{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahdi-amin/ApplicationWebsite/blob/master/POS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps Involved in the POS tagging example:\n",
        "\n",
        "\n",
        "*   Tokenize text (word_tokenize)\n",
        "*   apply pos_tag to above step that is nltk.pos_tag(tokenize_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "889iBVqykWCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK POS tagger is used to assign grammatical information of each word of the sentence. Installing, Importing and downloading all the packages of POS NLTK is complete."
      ],
      "metadata": {
        "id": "HEGRws4bktRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK POS Tags Examples are as below:"
      ],
      "metadata": {
        "id": "wDlgVdV9i4tE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CC --> coordinating conjunction (e.g and)\n",
        "\n",
        "CD -->  cardinal digit\n",
        "\n",
        "DT -->\tdeterminer (e.g the, that)\n",
        "\n",
        "EX --> \texistential there\n",
        "\n",
        "FW --> \tforeign word\n",
        "\n",
        "IN -->\tpreposition/subordinating conjunction\n",
        "\n",
        "JJ -->\tThis NLTK POS Tag is an adjective (large)\n",
        "\n",
        "JJR -->\tadjective, comparative (larger)\n",
        "\n",
        "JJS -->\tadjective, superlative (largest)\n",
        "\n",
        "LS \t--> list market\n",
        "\n",
        "MD \t--> modal (could, will)\n",
        "\n",
        "NN \t-->  noun, singular (cat, tree)\n",
        "\n",
        "NNS -->\tnoun plural (desks)\n",
        "\n",
        "NNP -->\tproper noun, singular (sarah)\n",
        "\n",
        "NNPS--> proper noun, plural (indians or americans)\n",
        "\n",
        "PDT --> predeterminer (all, both, half)\n",
        "\n",
        "POS --> possessive ending (parent\\ ‘s)\n",
        "\n",
        "PRP --> personal pronoun (hers, herself, him, himself)\n",
        "\n",
        "PRP$--> possessive pronoun (her, his, mine, my, our )\n",
        "\n",
        "RB \t--> adverb (occasionally, swiftly)\n",
        "\n",
        "RBR --> adverb, comparative (greater)\n",
        "\n",
        "RBS --> adverb, superlative (biggest)\n",
        "\n",
        "RP \t--> particle (about)\n",
        "\n",
        "TO  --> infinite marker (to)\n",
        "\n",
        "UH \t--> interjection (goodbye)\n",
        "\n",
        "VB  -->\tverb (ask)\n",
        "\n",
        "VBG --> verb gerund (judging)\n",
        "\n",
        "VBD --> verb past tense (pleaded)\n",
        "\n",
        "VBN --> verb past participle (reunified)\n",
        "\n",
        "VBP --> verb, present tense not 3rd person singular(wrap)\n",
        "\n",
        "VBZ --> verb, present tense with 3rd person singular (bases)\n",
        "\n",
        "WDT --> wh-determiner (that, what)\n",
        "\n",
        "WP  --> wh- pronoun (who)\n",
        "\n",
        "WRB --> wh- adverb (how)"
      ],
      "metadata": {
        "id": "w_6OKzHbciPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bqs5mFFDzdxc",
        "outputId": "4de18cff-5664-46d6-ccd9-6f2d408562d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "bnKGuOcxo_SO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f25ad75-88b0-4464-9775-a5249d7949a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Imagine going to your local hardware store and seeing a new kind of hammer on the shelf. You’ve heard about this hammer: It pounds faster and more accurately than others, and in the last few years it’s rendered many other hammers obsolete, at least for most uses. And there’s more! With a few tweaks — an attachment here, a twist there — the tool changes into a saw that can cut at least as fast and as accurately as any other option out there. In fact, some experts at the frontiers of tool development say this hammer might just herald the convergence of all tools into a single device.\n",
        "A similar story is playing out among the tools of artificial intelligence. That versatile new hammer is a kind of artificial neural network — a network of nodes that “learn” how to do some task by training on existing data — called a transformer. It was originally designed to handle language, but has recently begun impacting other AI domains.\n",
        "\n",
        "The transformer first appeared in 2017 in a paper that cryptically declared that “Attention Is All You Need.” In other approaches to AI, the system would first focus on local patches of input data and then build up to the whole. In a language model, for example, nearby words would first get grouped together. The transformer, by contrast, runs processes so that every element in the input data connects, or pays attention, to every other element. Researchers refer to this as “self-attention.” This means that as soon as it starts training, the transformer can see traces of the entire data set.\n",
        "\n",
        "Before transformers came along, progress on AI language tasks largely lagged behind developments in other areas. “In this deep learning revolution that happened in the past 10 years or so, natural language processing was sort of a latecomer,” said the computer scientist Anna Rumshisky of the University of Massachusetts, Lowell. “So NLP was, in a sense, behind computer vision. Transformers changed that.”\n",
        "\n",
        "Transformers quickly became the front-runner for applications like word recognition that focus on analyzing and predicting text. It led to a wave of tools, like OpenAI’s Generative Pre-trained Transformer 3 (GPT-3), which trains on hundreds of billions of words and generates consistent new text to an unsettling degree.\n",
        "\n",
        "The success of transformers prompted the AI crowd to ask what else they could do. The answer is unfolding now, as researchers report that transformers are proving surprisingly versatile. In some vision tasks, like image classification, neural nets that use transformers have become faster and more accurate than those that don’t. Emerging work in other AI areas — like processing multiple kinds of input at once, or planning tasks — suggests transformers can handle even more.\n",
        "\n",
        "“Transformers seem to really be quite transformational across many problems in machine learning, including computer vision,” said Vladimir Haltakov, who works on computer vision related to self-driving cars at BMW in Munich.\n",
        "\n",
        "Just 10 years ago, disparate subfields of AI had little to say to each other. But the arrival of transformers suggests the possibility of a convergence. “I think the transformer is so popular because it implies the potential to become universal,” said the computer scientist Atlas Wang of the University of Texas, Austin. “We have good reason to want to try transformers for the entire spectrum” of AI tasks.\n",
        "\n",
        "From Language to Vision\n",
        "\n",
        "One of the most promising steps toward expanding the range of transformers began just months after the release of “Attention Is All You Need.” Alexey Dosovitskiy, a computer scientist then at Google Brain Berlin, was working on computer vision, the AI subfield that focuses on teaching computers how to process and classify images. Like almost everyone else in the field, he worked with convolutional neural networks (CNNs), which for years had propelled all major leaps forward in deep learning and especially in computer vision.\n",
        "\n",
        "CNNs work by repeatedly applying filters to the pixels in an image to build up a recognition of features. It’s because of convolutions that photo apps can organize your library by faces or tell an avocado apart from a cloud. CNNs were considered indispensable to vision tasks.\n",
        "\n",
        "Dosovitskiy was working on one of the biggest challenges in the field, which was to scale up CNNs to train on ever-larger data sets representing images of ever-higher resolution without piling on the processing time. But then he watched transformers displace the previous go-to tools for nearly every AI task related to language. “We were clearly inspired by what was going on,” he said. “They were getting all these amazing results. We started wondering if we could do something similar in vision.” The idea made a certain kind of sense — after all, if transformers could handle big data sets of words, why not pictures?\n",
        "\n",
        "The eventual result was a network dubbed the Vision Transformer, or ViT, which the researchers presented at a conference in May 2021. The architecture of the model was nearly identical to that of the first transformer proposed in 2017, with only minor changes allowing it to analyze images instead of words. “Language tends to be discrete,” said Rumshisky, “so a lot of adaptations have to discretize the image.”\n",
        "\n",
        "The ViT team knew they couldn’t exactly mimic the language approach since self-attention on every pixel would be prohibitively expensive in computing time. Instead, they divided the larger image into square units, or tokens. The size is arbitrary, as the tokens could be made larger or smaller depending on the resolution of the original image (the default is 16 pixels on a side). But by processing pixels in groups, and applying self-attention to each, the ViT could quickly churn through enormous training data sets, spitting out increasingly accurate classifications.\n",
        "\n",
        "The transformer classified images with over 90% accuracy — a far better result than anything Dosovitskiy expected — propelling it quickly to the top of the pack at the ImageNet classification challenge, a seminal image recognition contest. ViT’s success suggested that maybe convolutions aren’t as fundamental to computer vision as researchers believed.\n",
        "\n",
        "“I think it is quite likely that CNNs will be replaced by vision transformers or derivatives thereof in the midterm future,” said Neil Houlsby of Google Brain Zurich, who worked with Dosovitskiy to develop ViT. Those future models may be pure transformers, he said, or approaches that add self-attention to existing models.\n",
        "\n",
        "Additional results bolster these predictions. Researchers routinely test their models for image classification on the ImageNet database, and at the start of 2022, an updated version of ViT was second only to a newer approach that combines CNNs with transformers. CNNs without transformers, the longtime champs, barely reached the top 10.\n",
        "\n",
        "How Transformers Work\n",
        "\n",
        "The ImageNet results demonstrated that transformers could compete with leading CNNs. But Maithra Raghu, a computer scientist at Google Brain’s Mountain View office in California, wanted to know if they “see” images the same way CNNs do. Neural nets are notorious for being indecipherable black boxes, but there are ways to peek inside — such as by examining the net’s input and output, layer by layer, to see how the training data flows through. Raghu’s group did essentially this, picking ViT apart.\n",
        "\n",
        "Her group identified ways in which self-attention leads to a different means of perception within the algorithm. Ultimately, a transformer’s power comes from the way it processes the encoded data of an image. “In CNNs, you start off being very local and slowly get a global perspective,” said Raghu. A CNN recognizes an image pixel by pixel, identifying features like corners or lines by building its way up from the local to the global. But in transformers, with self-attention, even the very first layer of information processing makes connections between distant image locations (just as with language). If a CNN’s approach is like starting at a single pixel and zooming out, a transformer slowly brings the whole fuzzy image into focus.\n",
        "\n",
        "This difference is simpler to understand in the realm of language, where transformers were first conceived. Consider these sentences: “The owl spied a squirrel. It tried to grab it with its talons but only got the end of its tail.” The structure of the second sentence is confusing: What do those “it”s refer to? A CNN that focuses only on the words immediately around the “it”s would struggle, but a transformer connecting every word to every other word could discern that the owl did the grabbing, and the squirrel lost part of its tail.\n",
        "\n",
        "Now that it was clear transformers processed images fundamentally differently from convolutional networks, researchers only grew more excited. The transformer’s versatility in converting data from a one-dimensional string, like a sentence, into a two-dimensional array, like an image, suggests that such a model could handle data of many other flavors. Wang, for example, thinks the transformer may be a big step toward achieving a kind of convergence of neural net architectures, resulting in a universal approach to computer vision — and perhaps to other AI tasks as well. “There are limitations to making it really happen, of course,” he said, “but if there is a model that can universalize, where you can put all kinds of data in one machine, then certainly that’s very fancy.”\n",
        "\n",
        "Convergence Coming\n",
        "\n",
        "Now researchers want to apply transformers to an even harder task: inventing new images. Language tools such as GPT-3 can generate new text based on their training data. In a paper presented last year, Wang combined two transformer models in an effort to do the same for images, a much harder problem. When the double transformer network trained on the faces of more than 200,000 celebrities, it synthesized new facial images at moderate resolution. The invented celebrities are impressively realistic and at least as convincing as those created by CNNs, according to the inception score, a standard way of evaluating images generated by a neural net.\n",
        "\n",
        "Wang argues that the transformer’s success in generating images is even more surprising than ViT’s prowess in image classification. “A generative model needs to synthesize, needs to be able to add information to look plausible,” he said. And as with classification, the transformer approach is replacing convolutional networks.\n",
        "\n",
        "Raghu and Wang see potential for new uses of transformers in multimodal processing — a model that can simultaneously handle multiple types of data, like raw images, video and language. “It was trickier to do before,” Raghu said, because of that siloed approach where each type of data had its own specialized model. But transformers suggest a way to combine multiple input sources. “There’s a whole realm of interesting applications, combining some of these different types of data and images.” For example, multimodal networks might power a system that reads a person’s lips in addition to listening to their voice. “You could have a rich representation of both language and image information,” Raghu said, “and in a much deeper way than was possible before.”\n",
        "\n",
        "Emerging work suggests a spectrum of new uses for transformers in other AI domains, including teaching robots to recognize human body movements, training machines to discern emotions in speech, and detecting stress levels in electrocardiograms. Another program with transformer components is AlphaFold, which made headlines last year for its ability to quickly predict protein structures — a task that used to require a decade of intensive analysis.\n",
        "\n",
        "The Trade-Off\n",
        "\n",
        " Even if transformers can help unite and improve the tools of AI, emerging technologies often come at a steep cost, and this one is no different. A transformer requires a higher outlay of computational power in the pre-training phase before it can beat the accuracy of its conventional competitors.\n",
        "\n",
        "That could be a problem. “People are always getting more and more interested in high-resolution images,” Wang said. That training expense could be a drawback to widespread implementation of transformers. However, Raghu sees the training hurdle as one that can be overcome simply enough with sophisticated filters and other tools.\n",
        "\n",
        "Wang also points out that even though visual transformers have ignited new efforts to push AI forward — including his own — many of the new models still incorporate the best parts of convolutions. That means future models are more likely to use both than to abandon CNNs entirely, he says.\n",
        "\n",
        "It also suggests the tantalizing prospect of some hybrid architecture that draws on the strengths of transformers in ways that today’s researchers can’t predict. “Perhaps we shouldn’t rush to the conclusion that the transformer will be the final model,” Wang said. But it’s increasingly likely that the transformer will at least be a part of whatever new super-tool comes to an AI shop near you.\"\"\""
      ],
      "metadata": {
        "id": "F9G0ggWxo547"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.tokenize.sent_tokenize(text)"
      ],
      "metadata": {
        "id": "SQtOQg2GEHgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-giV9dRpBJ4",
        "outputId": "0767740f-26cc-49ce-df0c-19ab580e12d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mrs. Hudson made a cup of tea.', 'She is a wonderful woman.']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [nltk.tokenize.word_tokenize(s) for s in sentences]\n",
        "\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8flmwNQpDsF",
        "outputId": "407dd1bb-5328-49d9-8afc-b8baa77163d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Mrs.', 'Hudson', 'made', 'a', 'cup', 'of', 'tea', '.'],\n",
              " ['She', 'is', 'a', 'wonderful', 'woman', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## analyzing"
      ],
      "metadata": {
        "id": "HG4LswKrILBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok = []\n",
        "for s in sentences:\n",
        "  nltk.tokenize.word_tokenize(s)\n",
        "  tok.append(s)\n",
        "print(tok)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZICZXsZEizZ",
        "outputId": "79e5c8d2-4a0d-4687-eeb4-fe3bad3da6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mrs. Hudson made a cup of tea.', 'She is a wonderful woman.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok = []\n",
        "tok1 = []\n",
        "for s in sentences:\n",
        "  nltk.tokenize.sent_tokenize(s)\n",
        "  tok.append(s)\n",
        "  for k in s:\n",
        "    nltk.tokenize.word_tokenize(k)\n",
        "    tok1.append(k) \n",
        "print(tok1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj1A6iCqF5EA",
        "outputId": "9457bed0-af9d-44ad-a301-4cc468954b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['M', 'r', 's', '.', ' ', 'H', 'u', 'd', 's', 'o', 'n', ' ', 'm', 'a', 'd', 'e', ' ', 'a', ' ', 'c', 'u', 'p', ' ', 'o', 'f', ' ', 't', 'e', 'a', '.', 'S', 'h', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'f', 'u', 'l', ' ', 'w', 'o', 'm', 'a', 'n', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tagging"
      ],
      "metadata": {
        "id": "89M95vTLIUv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "PosTokens = [nltk.pos_tag(e) for e in tokens]"
      ],
      "metadata": {
        "id": "MDUaTJhvpK0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9707f30d-b26e-49b8-e967-be136823ad13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PosTokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43w7qxzUpeSo",
        "outputId": "14dba5f3-1f66-4179-8738-94503b351c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Mrs.', 'NNP'),\n",
              "  ('Hudson', 'NNP'),\n",
              "  ('made', 'VBD'),\n",
              "  ('a', 'DT'),\n",
              "  ('cup', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('tea', 'NN'),\n",
              "  ('.', '.')],\n",
              " [('She', 'PRP'),\n",
              "  ('is', 'VBZ'),\n",
              "  ('a', 'DT'),\n",
              "  ('wonderful', 'JJ'),\n",
              "  ('woman', 'NN'),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "text = \"Mrs. Hudson made a cup of tea. She is a wonderful woman.\"\n",
        "lower_case = text.lower()\n",
        "tokens = nltk.word_tokenize(lower_case)\n",
        "tags = nltk.pos_tag(tokens)\n",
        "counts = Counter( tag for word,  tag in tags)\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJl1En5UzHIy",
        "outputId": "1fe90328-7e53-4e39-c274-275c2102fc4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'NN': 5, 'DT': 2, '.': 2, 'VBD': 1, 'IN': 1, 'PRP': 1, 'VBZ': 1, 'JJ': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "a = \"Mrs. Hudson made a cup of tea. She is a wonderful woman.\"\n",
        "words = nltk.tokenize.word_tokenize(a)\n",
        "fd = nltk.FreqDist(words)\n",
        "fd.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "xvP-YMUe1-V0",
        "outputId": "be3c6a2a-03b7-4212-a70e-283342080ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3e05bd7ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "6syducdifkBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "star_words = \"Mrs. Hudson made a cup of tea. She is a wonderful woman.\""
      ],
      "metadata": {
        "id": "lGx5yfWBftRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import RegexpParser\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phx2_F73LgEF",
        "outputId": "3209bfe0-f029-44a7-d3ef-24b99f858924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_words = re.sub(\"[^a-zA-Z]\", \" \", star_words)"
      ],
      "metadata": {
        "id": "lQjciuxILqgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-ZGDwxQuLvsv",
        "outputId": "43bb3496-1da5-4568-c070-1b352c0fa802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mrs  Hudson made a cup of tea  She is a wonderful woman '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_words.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZQHEKUkL-tu",
        "outputId": "0f2f37ea-0a2f-4a38-be79-5ac48eb3000c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mrs',\n",
              " 'Hudson',\n",
              " 'made',\n",
              " 'a',\n",
              " 'cup',\n",
              " 'of',\n",
              " 'tea',\n",
              " 'She',\n",
              " 'is',\n",
              " 'a',\n",
              " 'wonderful',\n",
              " 'woman']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\" \".join(clean_words.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jq12uSM6MIPA",
        "outputId": "9b9df9cc-7de8-4d95-f06f-c448a41f3193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mrs Hudson made a cup of tea She is a wonderful woman'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_words = \" \".join(clean_words.split())"
      ],
      "metadata": {
        "id": "UcdgXNRhMecZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.word_tokenize(clean_words) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKDsHHX-MkEX",
        "outputId": "b71ead28-b86b-481e-ea75-ffc93c5d8930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mrs',\n",
              " 'Hudson',\n",
              " 'made',\n",
              " 'a',\n",
              " 'cup',\n",
              " 'of',\n",
              " 'tea',\n",
              " 'She',\n",
              " 'is',\n",
              " 'a',\n",
              " 'wonderful',\n",
              " 'woman']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(clean_words) "
      ],
      "metadata": {
        "id": "OKxJ0XnfMrl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag(tokens) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdJsTJR7Mu7F",
        "outputId": "aa243113-d439-4629-e02b-8f4ab7273c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Mrs', 'NNP'),\n",
              " ('Hudson', 'NNP'),\n",
              " ('made', 'VBD'),\n",
              " ('a', 'DT'),\n",
              " ('cup', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('tea', 'NN'),\n",
              " ('She', 'PRP'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('wonderful', 'JJ'),\n",
              " ('woman', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tokens = pos_tag(tokens)  "
      ],
      "metadata": {
        "id": "CaUBHEVkM2gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import RegexpParser\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "clean_words = re.sub(\"[^a-zA-Z]\", \" \", star_words)\n",
        "\n",
        "#clean_words = clean_words.split()\n",
        "\n",
        "\n",
        "clean_words = \" \".join(clean_words.split()) # remove multiple whitespaces\n",
        "\n",
        "\n",
        "tokens = nltk.word_tokenize(clean_words) \n",
        "\n",
        "tokens\n",
        "pos_tokens = pos_tag(tokens) \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#clean_words\n",
        "tokens\n",
        "\"\"\"\n",
        "pos_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQN1hPwKgG1G",
        "outputId": "72da41dd-5def-457d-92e8-381651e12fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Mrs', 'NNP'),\n",
              " ('Hudson', 'NNP'),\n",
              " ('made', 'VBD'),\n",
              " ('a', 'DT'),\n",
              " ('cup', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('tea', 'NN'),\n",
              " ('She', 'PRP'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('wonderful', 'JJ'),\n",
              " ('woman', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to Demonstrate Use Case"
      ],
      "metadata": {
        "id": "GrBBNDaN00dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "text = \"learn php from guru99\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)\n",
        "tag = nltk.pos_tag(tokens)\n",
        "print(tag)\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "cp  =nltk.RegexpParser(grammar)\n",
        "result = cp.parse(tag)\n",
        "print(result)\n",
        "\n",
        "#result.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT5CrmU-00BT",
        "outputId": "a8904d1c-ca78-4237-8079-73a4283120dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['learn', 'php', 'from', 'guru99']\n",
            "[('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN')]\n",
            "(S (NP learn/JJ php/NN) from/IN (NP guru99/NN))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COUNTING POS TAGS"
      ],
      "metadata": {
        "id": "-EvcyBBN1lbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have discussed various pos_tag in the previous section. In this particular tutorial, you will study how to count these tags. Counting tags are crucial for text classification as well as preparing the features for the Natural language-based operations. I will be discussing with you the approach which guru99 followed while preparing code along with a discussion of output. Hope this will help you."
      ],
      "metadata": {
        "id": "hUhE96PN1svA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to count Tags:"
      ],
      "metadata": {
        "id": "G4Apdyme1xoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here first we will write working code and then we will write different steps to explain the code."
      ],
      "metadata": {
        "id": "hAyu1CXf11to"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To count the tags, you can use the package Counter from the collection’s module. A counter is a dictionary subclass which works on the principle of key-value operation. It is an unordered collection where elements are stored as a dictionary key while the count is their value.\n",
        "Import nltk which contains modules to tokenize the text.\n",
        "Write the text whose pos_tag you want to count.\n",
        "Some words are in upper case and some in lower case, so it is appropriate to transform all the words in the lower case before applying tokenization.\n",
        "Pass the words through word_tokenize from nltk.\n",
        "Calculate the pos_tag of each token\n",
        "Now comes the role of dictionary counter. We have imported in the code line 1. Words are the key and tags are the value and counter will count each tag total count present in the text. "
      ],
      "metadata": {
        "id": "HMghgLdy2Yq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency Distribution"
      ],
      "metadata": {
        "id": "JvyV4a5t2ys8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency Distribution is referred to as the number of times an outcome of an experiment occurs. It is used to find the frequency of each word occurring in a document. It uses FreqDistclass and defined by the nltk.probabilty module."
      ],
      "metadata": {
        "id": "Hfpv01a33VJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe the graph above. It corresponds to counting the occurrence of each word in the text. It helps in the study of text and further in implementing text-based sentimental analysis. In a nutshell, it can be concluded that nltk has a module for counting the occurrence of each word in the text which helps in preparing the stats of natural language features. It plays a significant role in finding the keywords in the text. You can also extract the text from the pdf using libraries like extract, PyPDF2 and feed the text to nlk.FreqDist."
      ],
      "metadata": {
        "id": "YzF4hzKspr6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key term is “tokenize.” After tokenizing, it checks for each word in a given paragraph or text document to determine that number of times it occurred. You do not need the NLTK toolkit for this. You can also do it with your own python programming skills. NLTK toolkit only provides a ready-to-use code for the various operations.\n",
        "\n",
        "Counting each word may not be much useful. Instead one should focus on collocation and bigrams which deals with a lot of words in a pair. These pairs identify useful keywords to better natural language features which can be fed to the machine. Please look below for their details."
      ],
      "metadata": {
        "id": "vF1Xa86Fpt-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collocations: Bigrams and Trigrams"
      ],
      "metadata": {
        "id": "NQy06pwqpzJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Collocations?"
      ],
      "metadata": {
        "id": "u9EYw8WYp5Nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collocations are the pairs of words occurring together many times in a document. It is calculated by the number of those pair occurring together to the overall word count of the document.\n",
        "\n",
        "Consider electromagnetic spectrum with words like ultraviolet rays, infrared rays.\n",
        "\n",
        "The words ultraviolet and rays are not used individually and hence can be treated as Collocation. Another example is the CT Scan. We don’t say CT and Scan separately, and hence they are also treated as collocation.\n",
        "\n",
        "We can say that finding collocations requires calculating the frequencies of words and their appearance in the context of other words. These specific collections of words require filtering to retain useful content terms. Each gram of words may then be scored according to some association measure, to determine the relative likelihood of each Ingram being a collocation."
      ],
      "metadata": {
        "id": "x_OKcwvNp9YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collocation can be categorized into two types-\n",
        "\n",
        "Bigrams combination of two words\n",
        "    \n",
        "Trigramscombinationof three words\n"
      ],
      "metadata": {
        "id": "dbCNeyIqqCXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigrams and Trigrams provide more meaningful and useful features for the feature extraction stage. These are especially useful in text-based sentimental analysis."
      ],
      "metadata": {
        "id": "zqZ_fzYuqNy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigrams Example Code"
      ],
      "metadata": {
        "id": "qmlN8mIOqRgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "text = \"Guru99 is a totally new kind of learning experience.\"\n",
        "Tokens = nltk.word_tokenize(text)\n",
        "output = list(nltk.bigrams(Tokens))\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI1kb5p0qUui",
        "outputId": "434e462a-2f27-4b15-e3c2-dd19949ae9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Guru99', 'is'), ('is', 'a'), ('a', 'totally'), ('totally', 'new'), ('new', 'kind'), ('kind', 'of'), ('of', 'learning'), ('learning', 'experience'), ('experience', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trigrams Example Code"
      ],
      "metadata": {
        "id": "e-PjLMDOq2U5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes it becomes important to see a pair of three words in the sentence for statistical analysis and frequency count. This again plays a crucial role in forming NLP (natural language processing features) as well as text-based sentimental prediction.\n"
      ],
      "metadata": {
        "id": "CRyMmFxpq6SZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same code is run for calculating the trigrams."
      ],
      "metadata": {
        "id": "EtEhtA1_q-Mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "text = \"Guru99 is a totally new kind of learning experience.\"\n",
        "Tokens = nltk.word_tokenize(text)\n",
        "output = list(nltk.trigrams(Tokens))\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTCe35Wlq3gO",
        "outputId": "25678b11-61cd-4d80-9395-bdb0bee0c5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Guru99', 'is', 'a'), ('is', 'a', 'totally'), ('a', 'totally', 'new'), ('totally', 'new', 'kind'), ('new', 'kind', 'of'), ('kind', 'of', 'learning'), ('of', 'learning', 'experience'), ('learning', 'experience', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tagging Sentences"
      ],
      "metadata": {
        "id": "_pknAbclrL_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tagging Sentence in a broader sense refers to the addition of labels of the verb, noun, etc., by the context of the sentence. Identification of POS tags is a complicated process. Thus generic tagging of POS is manually not possible as some words may have different (ambiguous) meanings according to the structure of the sentence. Conversion of text in the form of list is an important step before tagging as each word in the list is looped and counted for a particular tag. Please see the below code to understand it better"
      ],
      "metadata": {
        "id": "k2VdPDd3rNyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "text = \"Hello Guru99, You have to build a very good site, and I love visiting your site.\"\n",
        "sentence = nltk.sent_tokenize(text)\n",
        "for sent in sentence:\n",
        "\t print(nltk.pos_tag(nltk.word_tokenize(sent)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP4MsHRUrNQd",
        "outputId": "ae23a22b-85f7-470c-ffe5-aca0654f6aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 'NNP'), ('Guru99', 'NNP'), (',', ','), ('You', 'PRP'), ('have', 'VBP'), ('to', 'TO'), ('build', 'VB'), ('a', 'DT'), ('very', 'RB'), ('good', 'JJ'), ('site', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('love', 'VBP'), ('visiting', 'VBG'), ('your', 'PRP$'), ('site', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Explanation:"
      ],
      "metadata": {
        "id": "y4YLnuz4rcA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Code to import nltk (Natural language toolkit which contains submodules such as sentence tokenize and word tokenize.)\n",
        "\n",
        "Text whose tags are to be printed.\n",
        "Sentence Tokenization\n",
        "For loop is implemented where words are tokenized from sentence and tag of each word is printed as output.\n"
      ],
      "metadata": {
        "id": "nEkkNyfErhWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Corpus there are two types of POS taggers:"
      ],
      "metadata": {
        "id": "w2zprkAIrovu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Rule-Based\n",
        "\n",
        "Stochastic POS Taggers\n"
      ],
      "metadata": {
        "id": "PTGecZxUrrsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Rule-Based POS Tagger: For the words having ambiguous meaning, rule-based approach on the basis of contextual information is applied. It is done so by checking or analyzing the meaning of the preceding or the following word. Information is analyzed from the surrounding of the word or within itself. Therefore words are tagged by the grammatical rules of a particular language such as capitalization and punctuation. e.g., Brill’s tagger."
      ],
      "metadata": {
        "id": "ikdr-RTqrwf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Stochastic POS Tagger: Different approaches such as frequency or probability are applied under this method. If a word is mostly tagged with a particular tag in training set then in the test sentence it is given that particular tag. The word tag is dependent not only on its own tag but also on the previous tag. This method is not always accurate. Another way is to calculate the probability of occurrence of a specific tag in a sentence. Thus the final tag is calculated by checking the highest probability of a word with a particular tag."
      ],
      "metadata": {
        "id": "-jbI8xhSrz8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS tagging with Hidden Markov Model\n",
        "\n",
        "Tagging Problems can also be modeled using HMM. It treats input tokens to be observable sequence while tags are considered as hidden states and goal is to determine the hidden state sequence. For example x = x1,x2,…………,xn where x is a sequence of tokens while y = y1,y2,y3,y4………ynis the hidden sequence."
      ],
      "metadata": {
        "id": "sQArjuO-r2WI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How Hidden Markov Model (HMM) Works?"
      ],
      "metadata": {
        "id": "D5E6x-_Ur5pU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HMM uses join distribution which is P(x, y) where x is the input sequence/ token sequence and y is tag sequence.\n",
        "\n",
        "Tag Sequence for x will be argmaxy1….ynp(x1,x2,….xn,y1,y2,y3,…..). We have categorized tags from the text, but stats of such tags are vital. So the next part is counting these tags for statistical study."
      ],
      "metadata": {
        "id": "pnnHnSrnr848"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary"
      ],
      "metadata": {
        "id": "wDidh8N4r_Ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "POS Tagging in NLTK is a process to mark up the words in text format for a particular part of a speech based on its definition and context.\n",
        "\n",
        "Some NLTK POS tagging examples are: CC, CD, EX, JJ, MD, NNP, PDT, PRP$, TO, etc.\n",
        "\n",
        "POS tagger is used to assign grammatical information of each word of the sentence. Installing, Importing and downloading all the packages of Part of Speech tagging with NLTK is complete.\n",
        "\n",
        "Chunking in NLP is a process to take small pieces of information and group them into large units.\n",
        "There are no pre-defined rules, but you can combine them according to need and requirement.\n",
        "\n",
        "Chunking is used for entity detection. An entity is that part of the sentence by which machine get the value for any intention.\n",
        "\n",
        "Chunking is used to categorize different tokens into the same chunk.\n"
      ],
      "metadata": {
        "id": "EqpVUBLUsBRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⏰ End..."
      ],
      "metadata": {
        "id": "Qc81Pqj8sLjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = ('''\n",
        "    NP: {<DT>?<JJ>*<NN>} # NP\n",
        "    ''')\n",
        "chunkParser = nltk.RegexpParser(grammar)"
      ],
      "metadata": {
        "id": "oRePr0otjIBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = chunkParser.parse(pos_tokens)"
      ],
      "metadata": {
        "id": "TKStn9ZWgjDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for subtree in tree.subtrees():\n",
        "    print(subtree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHvNmGPOja5P",
        "outputId": "5d746ac0-5ec3-44fa-a7b0-8b40ca66a27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  Imagine/NNP\n",
            "  going/VBG\n",
            "  to/TO\n",
            "  your/PRP$\n",
            "  (NP local/JJ hardware/NN)\n",
            "  (NP store/NN)\n",
            "  and/CC\n",
            "  seeing/VBG\n",
            "  (NP a/DT new/JJ kind/NN)\n",
            "  of/IN\n",
            "  (NP hammer/NN)\n",
            "  on/IN\n",
            "  (NP the/DT shelf/NN)\n",
            "  You/PRP\n",
            "  ve/VBP\n",
            "  heard/VBN\n",
            "  about/IN\n",
            "  (NP this/DT hammer/NN)\n",
            "  It/PRP\n",
            "  pounds/VBZ\n",
            "  faster/RBR\n",
            "  and/CC\n",
            "  more/RBR\n",
            "  accurately/RB\n",
            "  than/IN\n",
            "  others/NNS\n",
            "  and/CC\n",
            "  in/IN\n",
            "  the/DT\n",
            "  last/JJ\n",
            "  few/JJ\n",
            "  years/NNS\n",
            "  it/PRP\n",
            "  s/VBD\n",
            "  rendered/VBN\n",
            "  many/JJ\n",
            "  other/JJ\n",
            "  hammers/NNS\n",
            "  obsolete/VBP\n",
            "  at/IN\n",
            "  least/JJS\n",
            "  for/IN\n",
            "  most/JJS\n",
            "  uses/NNS\n",
            "  And/CC\n",
            "  there/EX\n",
            "  s/VB\n",
            "  more/JJR\n",
            "  With/IN\n",
            "  a/DT\n",
            "  few/JJ\n",
            "  tweaks/NNS\n",
            "  (NP an/DT attachment/NN)\n",
            "  here/RB\n",
            "  (NP a/DT twist/NN)\n",
            "  there/EX\n",
            "  (NP the/DT tool/NN)\n",
            "  changes/NNS\n",
            "  into/IN\n",
            "  (NP a/DT saw/NN)\n",
            "  that/WDT\n",
            "  can/MD\n",
            "  cut/VB\n",
            "  at/IN\n",
            "  least/JJS\n",
            "  as/IN\n",
            "  fast/JJ\n",
            "  and/CC\n",
            "  as/RB\n",
            "  accurately/RB\n",
            "  as/IN\n",
            "  (NP any/DT other/JJ option/NN)\n",
            "  out/IN\n",
            "  there/RB\n",
            "  In/IN\n",
            "  (NP fact/NN)\n",
            "  some/DT\n",
            "  experts/NNS\n",
            "  at/IN\n",
            "  the/DT\n",
            "  frontiers/NNS\n",
            "  of/IN\n",
            "  (NP tool/NN)\n",
            "  (NP development/NN)\n",
            "  say/VBP\n",
            "  (NP this/DT hammer/NN)\n",
            "  might/MD\n",
            "  just/RB\n",
            "  herald/VB\n",
            "  (NP the/DT convergence/NN)\n",
            "  of/IN\n",
            "  all/DT\n",
            "  tools/NNS\n",
            "  into/IN\n",
            "  (NP a/DT single/JJ device/NN)\n",
            "  A/NNP\n",
            "  (NP similar/JJ story/NN)\n",
            "  is/VBZ\n",
            "  playing/VBG\n",
            "  out/RP\n",
            "  among/IN\n",
            "  the/DT\n",
            "  tools/NNS\n",
            "  of/IN\n",
            "  (NP artificial/JJ intelligence/NN)\n",
            "  That/IN\n",
            "  (NP versatile/JJ new/JJ hammer/NN)\n",
            "  is/VBZ\n",
            "  (NP a/DT kind/NN)\n",
            "  of/IN\n",
            "  (NP artificial/JJ neural/JJ network/NN)\n",
            "  (NP a/DT network/NN)\n",
            "  of/IN\n",
            "  nodes/NNS\n",
            "  that/WDT\n",
            "  learn/VBP\n",
            "  how/WRB\n",
            "  to/TO\n",
            "  do/VB\n",
            "  (NP some/DT task/NN)\n",
            "  by/IN\n",
            "  training/VBG\n",
            "  on/IN\n",
            "  existing/VBG\n",
            "  data/NNS\n",
            "  called/VBD\n",
            "  (NP a/DT transformer/NN)\n",
            "  It/PRP\n",
            "  was/VBD\n",
            "  originally/RB\n",
            "  designed/VBN\n",
            "  to/TO\n",
            "  handle/VB\n",
            "  (NP language/NN)\n",
            "  but/CC\n",
            "  has/VBZ\n",
            "  recently/RB\n",
            "  begun/VBN\n",
            "  impacting/VBG\n",
            "  other/JJ\n",
            "  AI/NNP\n",
            "  domains/VBZ\n",
            "  (NP The/DT transformer/NN)\n",
            "  first/RB\n",
            "  appeared/VBD\n",
            "  in/IN\n",
            "  in/IN\n",
            "  (NP a/DT paper/NN)\n",
            "  that/WDT\n",
            "  cryptically/RB\n",
            "  declared/VBD\n",
            "  that/IN\n",
            "  Attention/NNP\n",
            "  Is/VBZ\n",
            "  All/NNP\n",
            "  You/PRP\n",
            "  Need/VBP\n",
            "  In/IN\n",
            "  other/JJ\n",
            "  approaches/NNS\n",
            "  to/TO\n",
            "  AI/VB\n",
            "  (NP the/DT system/NN)\n",
            "  would/MD\n",
            "  first/RB\n",
            "  focus/VB\n",
            "  on/IN\n",
            "  local/JJ\n",
            "  patches/NNS\n",
            "  of/IN\n",
            "  (NP input/NN)\n",
            "  data/NNS\n",
            "  and/CC\n",
            "  then/RB\n",
            "  build/VB\n",
            "  up/RP\n",
            "  to/TO\n",
            "  (NP the/DT whole/NN)\n",
            "  In/IN\n",
            "  (NP a/DT language/NN)\n",
            "  (NP model/NN)\n",
            "  for/IN\n",
            "  (NP example/NN)\n",
            "  nearby/JJ\n",
            "  words/NNS\n",
            "  would/MD\n",
            "  first/RB\n",
            "  get/VB\n",
            "  grouped/VBN\n",
            "  together/RB\n",
            "  (NP The/DT transformer/NN)\n",
            "  by/IN\n",
            "  (NP contrast/NN)\n",
            "  runs/NNS\n",
            "  processes/VBZ\n",
            "  so/RB\n",
            "  that/IN\n",
            "  (NP every/DT element/NN)\n",
            "  in/IN\n",
            "  (NP the/DT input/NN)\n",
            "  data/NNS\n",
            "  connects/NNS\n",
            "  or/CC\n",
            "  pays/NNS\n",
            "  (NP attention/NN)\n",
            "  to/TO\n",
            "  (NP every/DT other/JJ element/NN)\n",
            "  Researchers/NNP\n",
            "  refer/VBP\n",
            "  to/TO\n",
            "  this/DT\n",
            "  as/IN\n",
            "  (NP self/NN)\n",
            "  (NP attention/NN)\n",
            "  This/DT\n",
            "  means/VBZ\n",
            "  that/IN\n",
            "  as/RB\n",
            "  soon/RB\n",
            "  as/IN\n",
            "  it/PRP\n",
            "  starts/VBZ\n",
            "  training/VBG\n",
            "  (NP the/DT transformer/NN)\n",
            "  can/MD\n",
            "  see/VB\n",
            "  traces/NNS\n",
            "  of/IN\n",
            "  the/DT\n",
            "  entire/JJ\n",
            "  data/NNS\n",
            "  (NP set/NN)\n",
            "  Before/IN\n",
            "  transformers/NNS\n",
            "  came/VBD\n",
            "  along/RB\n",
            "  (NP progress/NN)\n",
            "  on/IN\n",
            "  AI/NNP\n",
            "  (NP language/NN)\n",
            "  tasks/NNS\n",
            "  largely/RB\n",
            "  lagged/VBD\n",
            "  behind/IN\n",
            "  developments/NNS\n",
            "  in/IN\n",
            "  other/JJ\n",
            "  areas/NNS\n",
            "  In/IN\n",
            "  (NP this/DT deep/JJ learning/NN)\n",
            "  (NP revolution/NN)\n",
            "  that/WDT\n",
            "  happened/VBD\n",
            "  in/IN\n",
            "  the/DT\n",
            "  past/JJ\n",
            "  years/NNS\n",
            "  or/CC\n",
            "  so/RB\n",
            "  (NP natural/JJ language/NN)\n",
            "  (NP processing/NN)\n",
            "  was/VBD\n",
            "  (NP sort/NN)\n",
            "  of/IN\n",
            "  (NP a/DT latecomer/NN)\n",
            "  said/VBD\n",
            "  (NP the/DT computer/NN)\n",
            "  (NP scientist/NN)\n",
            "  Anna/NNP\n",
            "  Rumshisky/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  University/NNP\n",
            "  of/IN\n",
            "  Massachusetts/NNP\n",
            "  Lowell/NNP\n",
            "  So/NNP\n",
            "  NLP/NNP\n",
            "  was/VBD\n",
            "  in/IN\n",
            "  (NP a/DT sense/NN)\n",
            "  behind/IN\n",
            "  (NP computer/NN)\n",
            "  (NP vision/NN)\n",
            "  Transformers/NNP\n",
            "  changed/VBD\n",
            "  that/IN\n",
            "  Transformers/NNP\n",
            "  quickly/RB\n",
            "  became/VBD\n",
            "  (NP the/DT front/NN)\n",
            "  (NP runner/NN)\n",
            "  for/IN\n",
            "  applications/NNS\n",
            "  like/IN\n",
            "  (NP word/NN)\n",
            "  (NP recognition/NN)\n",
            "  that/WDT\n",
            "  (NP focus/NN)\n",
            "  on/IN\n",
            "  analyzing/VBG\n",
            "  and/CC\n",
            "  predicting/VBG\n",
            "  (NP text/NN)\n",
            "  It/PRP\n",
            "  led/VBD\n",
            "  to/TO\n",
            "  (NP a/DT wave/NN)\n",
            "  of/IN\n",
            "  tools/NNS\n",
            "  like/IN\n",
            "  OpenAI/NNP\n",
            "  s/VBP\n",
            "  Generative/NNP\n",
            "  Pre/NNP\n",
            "  trained/VBD\n",
            "  Transformer/NNP\n",
            "  GPT/NNP\n",
            "  which/WDT\n",
            "  trains/VBZ\n",
            "  on/IN\n",
            "  hundreds/NNS\n",
            "  of/IN\n",
            "  billions/NNS\n",
            "  of/IN\n",
            "  words/NNS\n",
            "  and/CC\n",
            "  generates/NNS\n",
            "  (NP consistent/JJ new/JJ text/NN)\n",
            "  to/TO\n",
            "  (NP an/DT unsettling/JJ degree/NN)\n",
            "  (NP The/DT success/NN)\n",
            "  of/IN\n",
            "  transformers/NNS\n",
            "  prompted/VBD\n",
            "  the/DT\n",
            "  AI/NNP\n",
            "  (NP crowd/NN)\n",
            "  to/TO\n",
            "  ask/VB\n",
            "  what/WP\n",
            "  else/IN\n",
            "  they/PRP\n",
            "  could/MD\n",
            "  do/VB\n",
            "  (NP The/DT answer/NN)\n",
            "  is/VBZ\n",
            "  unfolding/VBG\n",
            "  now/RB\n",
            "  as/IN\n",
            "  researchers/NNS\n",
            "  report/VBP\n",
            "  that/IN\n",
            "  transformers/NNS\n",
            "  are/VBP\n",
            "  proving/VBG\n",
            "  surprisingly/RB\n",
            "  (NP versatile/NN)\n",
            "  In/IN\n",
            "  (NP some/DT vision/NN)\n",
            "  tasks/NNS\n",
            "  like/IN\n",
            "  (NP image/NN)\n",
            "  (NP classification/NN)\n",
            "  neural/JJ\n",
            "  nets/NNS\n",
            "  that/WDT\n",
            "  use/VBP\n",
            "  transformers/NNS\n",
            "  have/VBP\n",
            "  become/VBN\n",
            "  faster/RBR\n",
            "  and/CC\n",
            "  more/RBR\n",
            "  accurate/JJ\n",
            "  than/IN\n",
            "  those/DT\n",
            "  that/IN\n",
            "  don/VBP\n",
            "  t/JJ\n",
            "  Emerging/NNP\n",
            "  (NP work/NN)\n",
            "  in/IN\n",
            "  other/JJ\n",
            "  AI/NNP\n",
            "  areas/NNS\n",
            "  like/IN\n",
            "  processing/VBG\n",
            "  multiple/JJ\n",
            "  kinds/NNS\n",
            "  of/IN\n",
            "  (NP input/NN)\n",
            "  at/IN\n",
            "  once/RB\n",
            "  or/CC\n",
            "  planning/VBG\n",
            "  tasks/NNS\n",
            "  suggests/VBZ\n",
            "  transformers/NNS\n",
            "  can/MD\n",
            "  handle/VB\n",
            "  even/RB\n",
            "  more/JJR\n",
            "  Transformers/NNS\n",
            "  seem/VBP\n",
            "  to/TO\n",
            "  really/RB\n",
            "  be/VB\n",
            "  quite/RB\n",
            "  transformational/JJ\n",
            "  across/IN\n",
            "  many/JJ\n",
            "  problems/NNS\n",
            "  in/IN\n",
            "  (NP machine/NN)\n",
            "  (NP learning/NN)\n",
            "  including/VBG\n",
            "  (NP computer/NN)\n",
            "  (NP vision/NN)\n",
            "  said/VBD\n",
            "  Vladimir/NNP\n",
            "  Haltakov/NNP\n",
            "  who/WP\n",
            "  works/VBZ\n",
            "  on/IN\n",
            "  (NP computer/NN)\n",
            "  (NP vision/NN)\n",
            "  related/VBN\n",
            "  to/TO\n",
            "  self/VB\n",
            "  driving/VBG\n",
            "  cars/NNS\n",
            "  at/IN\n",
            "  BMW/NNP\n",
            "  in/IN\n",
            "  Munich/NNP\n",
            "  Just/NNP\n",
            "  years/NNS\n",
            "  ago/RB\n",
            "  disparate/VBP\n",
            "  subfields/NNS\n",
            "  of/IN\n",
            "  AI/NNP\n",
            "  had/VBD\n",
            "  little/JJ\n",
            "  to/TO\n",
            "  say/VB\n",
            "  to/TO\n",
            "  each/DT\n",
            "  other/JJ\n",
            "  But/CC\n",
            "  (NP the/DT arrival/NN)\n",
            "  of/IN\n",
            "  transformers/NNS\n",
            "  suggests/VBZ\n",
            "  (NP the/DT possibility/NN)\n",
            "  of/IN\n",
            "  (NP a/DT convergence/NN)\n",
            "  I/PRP\n",
            "  think/VBP\n",
            "  (NP the/DT transformer/NN)\n",
            "  is/VBZ\n",
            "  so/RB\n",
            "  popular/JJ\n",
            "  because/IN\n",
            "  it/PRP\n",
            "  implies/VBZ\n",
            "  the/DT\n",
            "  potential/JJ\n",
            "  to/TO\n",
            "  become/VB\n",
            "  (NP universal/NN)\n",
            "  said/VBD\n",
            "  (NP the/DT computer/NN)\n",
            "  (NP scientist/NN)\n",
            "  Atlas/NNP\n",
            "  Wang/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  University/NNP\n",
            "  of/IN\n",
            "  Texas/NNP\n",
            "  Austin/NNP\n",
            "  We/PRP\n",
            "  have/VBP\n",
            "  (NP good/JJ reason/NN)\n",
            "  to/TO\n",
            "  want/VB\n",
            "  to/TO\n",
            "  try/VB\n",
            "  transformers/NNS\n",
            "  for/IN\n",
            "  (NP the/DT entire/JJ spectrum/NN)\n",
            "  of/IN\n",
            "  AI/NNP\n",
            "  tasks/NNS\n",
            "  From/NNP\n",
            "  Language/NNP\n",
            "  to/TO\n",
            "  Vision/NNP\n",
            "  One/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  most/RBS\n",
            "  promising/JJ\n",
            "  steps/NNS\n",
            "  toward/IN\n",
            "  expanding/VBG\n",
            "  (NP the/DT range/NN)\n",
            "  of/IN\n",
            "  transformers/NNS\n",
            "  began/VBD\n",
            "  just/RB\n",
            "  months/NNS\n",
            "  after/IN\n",
            "  (NP the/DT release/NN)\n",
            "  of/IN\n",
            "  Attention/NNP\n",
            "  Is/NNP\n",
            "  All/NNP\n",
            "  You/PRP\n",
            "  Need/VBP\n",
            "  Alexey/NNP\n",
            "  Dosovitskiy/NNP\n",
            "  (NP a/DT computer/NN)\n",
            "  (NP scientist/NN)\n",
            "  then/RB\n",
            "  at/IN\n",
            "  Google/NNP\n",
            "  Brain/NNP\n",
            "  Berlin/NNP\n",
            "  was/VBD\n",
            "  working/VBG\n",
            "  on/IN\n",
            "  (NP computer/NN)\n",
            "  (NP vision/NN)\n",
            "  the/DT\n",
            "  AI/NNP\n",
            "  subfield/VBD\n",
            "  that/DT\n",
            "  focuses/VBZ\n",
            "  on/IN\n",
            "  teaching/VBG\n",
            "  computers/NNS\n",
            "  how/WRB\n",
            "  to/TO\n",
            "  process/VB\n",
            "  and/CC\n",
            "  classify/VB\n",
            "  images/NNS\n",
            "  Like/IN\n",
            "  almost/RB\n",
            "  (NP everyone/NN)\n",
            "  else/RB\n",
            "  in/IN\n",
            "  (NP the/DT field/NN)\n",
            "  he/PRP\n",
            "  worked/VBD\n",
            "  with/IN\n",
            "  convolutional/JJ\n",
            "  neural/JJ\n",
            "  networks/NNS\n",
            "  CNNs/NNP\n",
            "  which/WDT\n",
            "  for/IN\n",
            "  years/NNS\n",
            "  had/VBD\n",
            "  propelled/VBN\n",
            "  all/DT\n",
            "  major/JJ\n",
            "  leaps/NNS\n",
            "  forward/RB\n",
            "  in/IN\n",
            "  (NP deep/JJ learning/NN)\n",
            "  and/CC\n",
            "  especially/RB\n",
            "  in/IN\n",
            "  (NP computer/NN)\n",
            "  (NP vision/NN)\n",
            "  CNNs/NNP\n",
            "  (NP work/NN)\n",
            "  by/IN\n",
            "  repeatedly/RB\n",
            "  applying/VBG\n",
            "  filters/NNS\n",
            "  to/TO\n",
            "  the/DT\n",
            "  pixels/NNS\n",
            "  in/IN\n",
            "  (NP an/DT image/NN)\n",
            "  to/TO\n",
            "  build/VB\n",
            "  up/RP\n",
            "  (NP a/DT recognition/NN)\n",
            "  of/IN\n",
            "  features/NNS\n",
            "  It/PRP\n",
            "  s/VBZ\n",
            "  because/IN\n",
            "  of/IN\n",
            "  convolutions/NNS\n",
            "  that/WDT\n",
            "  photo/VBP\n",
            "  apps/NNS\n",
            "  can/MD\n",
            "  organize/VB\n",
            "  your/PRP$\n",
            "  (NP library/NN)\n",
            "  by/IN\n",
            "  faces/VBZ\n",
            "  or/CC\n",
            "  tell/VB\n",
            "  (NP an/DT avocado/JJ apart/NN)\n",
            "  from/IN\n",
            "  (NP a/DT cloud/NN)\n",
            "  CNNs/NNP\n",
            "  were/VBD\n",
            "  considered/VBN\n",
            "  indispensable/JJ\n",
            "  to/TO\n",
            "  (NP vision/NN)\n",
            "  tasks/NNS\n",
            "  Dosovitskiy/NNP\n",
            "  was/VBD\n",
            "  working/VBG\n",
            "  on/IN\n",
            "  one/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  biggest/JJS\n",
            "  challenges/NNS\n",
            "  in/IN\n",
            "  (NP the/DT field/NN)\n",
            "  which/WDT\n",
            "  was/VBD\n",
            "  to/TO\n",
            "  scale/VB\n",
            "  up/RP\n",
            "  CNNs/NNP\n",
            "  to/TO\n",
            "  train/VB\n",
            "  on/IN\n",
            "  ever/RB\n",
            "  larger/JJR\n",
            "  data/NNS\n",
            "  sets/NNS\n",
            "  representing/VBG\n",
            "  images/NNS\n",
            "  of/IN\n",
            "  ever/RB\n",
            "  higher/JJR\n",
            "  (NP resolution/NN)\n",
            "  without/IN\n",
            "  piling/VBG\n",
            "  on/IN\n",
            "  (NP the/DT processing/NN)\n",
            "  (NP time/NN)\n",
            "  But/CC\n",
            "  then/RB\n",
            "  he/PRP\n",
            "  watched/VBD\n",
            "  transformers/NNS\n",
            "  displace/VBP\n",
            "  (NP the/DT previous/JJ go/NN)\n",
            "  to/TO\n",
            "  tools/VB\n",
            "  for/IN\n",
            "  nearly/RB\n",
            "  every/DT\n",
            "  AI/NNP\n",
            "  (NP task/NN)\n",
            "  related/VBN\n",
            "  to/TO\n",
            "  (NP language/NN)\n",
            "  We/PRP\n",
            "  were/VBD\n",
            "  clearly/RB\n",
            "  inspired/VBN\n",
            "  by/IN\n",
            "  what/WP\n",
            "  was/VBD\n",
            "  going/VBG\n",
            "  on/IN\n",
            "  he/PRP\n",
            "  said/VBD\n",
            "  They/PRP\n",
            "  were/VBD\n",
            "  getting/VBG\n",
            "  all/PDT\n",
            "  these/DT\n",
            "  amazing/JJ\n",
            "  results/NNS\n",
            "  We/PRP\n",
            "  started/VBD\n",
            "  wondering/VBG\n",
            "  if/IN\n",
            "  we/PRP\n",
            "  could/MD\n",
            "  do/VB\n",
            "  (NP something/NN)\n",
            "  similar/JJ\n",
            "  in/IN\n",
            "  (NP vision/NN)\n",
            "  (NP The/DT idea/NN)\n",
            "  made/VBD\n",
            "  (NP a/DT certain/JJ kind/NN)\n",
            "  of/IN\n",
            "  (NP sense/NN)\n",
            "  after/IN\n",
            "  all/DT\n",
            "  if/IN\n",
            "  transformers/NNS\n",
            "  could/MD\n",
            "  handle/VB\n",
            "  big/JJ\n",
            "  data/NNS\n",
            "  sets/NNS\n",
            "  of/IN\n",
            "  words/NNS\n",
            "  why/WRB\n",
            "  not/RB\n",
            "  pictures/VBZ\n",
            "  (NP The/DT eventual/JJ result/NN)\n",
            "  was/VBD\n",
            "  (NP a/DT network/NN)\n",
            "  dubbed/VBD\n",
            "  the/DT\n",
            "  Vision/NNP\n",
            "  Transformer/NNP\n",
            "  or/CC\n",
            "  ViT/NNP\n",
            "  which/WDT\n",
            "  the/DT\n",
            "  researchers/NNS\n",
            "  presented/VBN\n",
            "  at/IN\n",
            "  (NP a/DT conference/NN)\n",
            "  in/IN\n",
            "  May/NNP\n",
            "  (NP The/DT architecture/NN)\n",
            "  of/IN\n",
            "  (NP the/DT model/NN)\n",
            "  was/VBD\n",
            "  nearly/RB\n",
            "  identical/JJ\n",
            "  to/TO\n",
            "  that/DT\n",
            "  of/IN\n",
            "  (NP the/DT first/JJ transformer/NN)\n",
            "  proposed/VBN\n",
            "  in/IN\n",
            "  with/IN\n",
            "  only/JJ\n",
            "  minor/JJ\n",
            "  changes/NNS\n",
            "  allowing/VBG\n",
            "  it/PRP\n",
            "  to/TO\n",
            "  analyze/VB\n",
            "  images/NNS\n",
            "  instead/RB\n",
            "  of/IN\n",
            "  words/NNS\n",
            "  Language/NNP\n",
            "  tends/VBZ\n",
            "  to/TO\n",
            "  be/VB\n",
            "  discrete/VBN\n",
            "  said/VBD\n",
            "  Rumshisky/NNP\n",
            "  so/IN\n",
            "  (NP a/DT lot/NN)\n",
            "  of/IN\n",
            "  adaptations/NNS\n",
            "  have/VBP\n",
            "  to/TO\n",
            "  discretize/VB\n",
            "  (NP the/DT image/NN)\n",
            "  The/DT\n",
            "  ViT/NNP\n",
            "  (NP team/NN)\n",
            "  knew/VBD\n",
            "  they/PRP\n",
            "  couldn/VBP\n",
            "  t/RB\n",
            "  exactly/RB\n",
            "  mimic/VBZ\n",
            "  (NP the/DT language/NN)\n",
            "  (NP approach/NN)\n",
            "  since/IN\n",
            "  (NP self/JJ attention/NN)\n",
            "  on/IN\n",
            "  (NP every/DT pixel/NN)\n",
            "  would/MD\n",
            "  be/VB\n",
            "  prohibitively/RB\n",
            "  expensive/JJ\n",
            "  in/IN\n",
            "  computing/VBG\n",
            "  (NP time/NN)\n",
            "  Instead/RB\n",
            "  they/PRP\n",
            "  divided/VBD\n",
            "  the/DT\n",
            "  larger/JJR\n",
            "  (NP image/NN)\n",
            "  into/IN\n",
            "  square/JJ\n",
            "  units/NNS\n",
            "  or/CC\n",
            "  tokens/VBZ\n",
            "  (NP The/DT size/NN)\n",
            "  is/VBZ\n",
            "  arbitrary/JJ\n",
            "  as/IN\n",
            "  the/DT\n",
            "  tokens/NNS\n",
            "  could/MD\n",
            "  be/VB\n",
            "  made/VBN\n",
            "  larger/JJR\n",
            "  or/CC\n",
            "  smaller/JJR\n",
            "  depending/VBG\n",
            "  on/IN\n",
            "  (NP the/DT resolution/NN)\n",
            "  of/IN\n",
            "  (NP the/DT original/JJ image/NN)\n",
            "  (NP the/DT default/NN)\n",
            "  is/VBZ\n",
            "  pixels/NNS\n",
            "  on/IN\n",
            "  (NP a/DT side/NN)\n",
            "  But/CC\n",
            "  by/IN\n",
            "  processing/VBG\n",
            "  pixels/NNS\n",
            "  in/IN\n",
            "  groups/NNS\n",
            "  and/CC\n",
            "  applying/VBG\n",
            "  self/PRP\n",
            "  (NP attention/NN)\n",
            "  to/TO\n",
            "  each/DT\n",
            "  the/DT\n",
            "  ViT/NNP\n",
            "  could/MD\n",
            "  quickly/RB\n",
            "  churn/VB\n",
            "  through/IN\n",
            "  (NP enormous/JJ training/NN)\n",
            "  data/NNS\n",
            "  sets/NNS\n",
            "  spitting/VBG\n",
            "  out/RP\n",
            "  increasingly/RB\n",
            "  accurate/JJ\n",
            "  classifications/NNS\n",
            "  (NP The/DT transformer/NN)\n",
            "  classified/VBD\n",
            "  images/NNS\n",
            "  with/IN\n",
            "  over/IN\n",
            "  (NP accuracy/NN)\n",
            "  a/DT\n",
            "  far/RB\n",
            "  better/RBR\n",
            "  (NP result/NN)\n",
            "  than/IN\n",
            "  (NP anything/NN)\n",
            "  Dosovitskiy/NNP\n",
            "  expected/VBD\n",
            "  propelling/VBG\n",
            "  it/PRP\n",
            "  quickly/RB\n",
            "  to/TO\n",
            "  (NP the/DT top/NN)\n",
            "  of/IN\n",
            "  (NP the/DT pack/NN)\n",
            "  at/IN\n",
            "  the/DT\n",
            "  ImageNet/NNP\n",
            "  (NP classification/NN)\n",
            "  (NP challenge/NN)\n",
            "  (NP a/DT seminal/JJ image/NN)\n",
            "  (NP recognition/NN)\n",
            "  (NP contest/NN)\n",
            "  ViT/NNP\n",
            "  (NP s/NN)\n",
            "  (NP success/NN)\n",
            "  suggested/VBD\n",
            "  that/IN\n",
            "  maybe/RB\n",
            "  convolutions/NNS\n",
            "  aren/VBP\n",
            "  (NP t/NN)\n",
            "  as/IN\n",
            "  fundamental/JJ\n",
            "  to/TO\n",
            "  (NP computer/NN)\n",
            "  (NP vision/NN)\n",
            "  as/IN\n",
            "  researchers/NNS\n",
            "  believed/VBD\n",
            "  I/PRP\n",
            "  think/VBP\n",
            "  it/PRP\n",
            "  is/VBZ\n",
            "  quite/RB\n",
            "  likely/JJ\n",
            "  that/IN\n",
            "  CNNs/NNP\n",
            "  will/MD\n",
            "  be/VB\n",
            "  replaced/VBN\n",
            "  by/IN\n",
            "  (NP vision/NN)\n",
            "  transformers/NNS\n",
            "  or/CC\n",
            "  derivatives/NNS\n",
            "  thereof/VBP\n",
            "  in/IN\n",
            "  (NP the/DT midterm/JJ future/NN)\n",
            "  said/VBD\n",
            "  Neil/NNP\n",
            "  Houlsby/NNP\n",
            "  of/IN\n",
            "  Google/NNP\n",
            "  Brain/NNP\n",
            "  Zurich/NNP\n",
            "  who/WP\n",
            "  worked/VBD\n",
            "  with/IN\n",
            "  Dosovitskiy/NNP\n",
            "  to/TO\n",
            "  develop/VB\n",
            "  ViT/NNP\n",
            "  Those/DT\n",
            "  future/JJ\n",
            "  models/NNS\n",
            "  may/MD\n",
            "  be/VB\n",
            "  pure/JJ\n",
            "  transformers/NNS\n",
            "  he/PRP\n",
            "  said/VBD\n",
            "  or/CC\n",
            "  approaches/NNS\n",
            "  that/IN\n",
            "  add/VBP\n",
            "  (NP self/JJ attention/NN)\n",
            "  to/TO\n",
            "  existing/VBG\n",
            "  models/NNS\n",
            "  Additional/NNP\n",
            "  results/NNS\n",
            "  bolster/VB\n",
            "  these/DT\n",
            "  predictions/NNS\n",
            "  Researchers/NNP\n",
            "  routinely/RB\n",
            "  test/VBP\n",
            "  their/PRP$\n",
            "  models/NNS\n",
            "  for/IN\n",
            "  (NP image/NN)\n",
            "  (NP classification/NN)\n",
            "  on/IN\n",
            "  the/DT\n",
            "  ImageNet/NNP\n",
            "  (NP database/NN)\n",
            "  and/CC\n",
            "  at/IN\n",
            "  (NP the/DT start/NN)\n",
            "  of/IN\n",
            "  (NP an/DT updated/JJ version/NN)\n",
            "  of/IN\n",
            "  ViT/NNP\n",
            "  was/VBD\n",
            "  second/JJ\n",
            "  only/RB\n",
            "  to/TO\n",
            "  a/DT\n",
            "  newer/JJR\n",
            "  (NP approach/NN)\n",
            "  that/IN\n",
            "  combines/VBZ\n",
            "  CNNs/NNP\n",
            "  with/IN\n",
            "  transformers/NNS\n",
            "  CNNs/NNP\n",
            "  without/IN\n",
            "  transformers/NNS\n",
            "  the/DT\n",
            "  longtime/JJ\n",
            "  champs/NNS\n",
            "  barely/RB\n",
            "  reached/VBD\n",
            "  the/DT\n",
            "  top/JJ\n",
            "  How/NNP\n",
            "  Transformers/NNP\n",
            "  Work/VBP\n",
            "  The/DT\n",
            "  ImageNet/NNP\n",
            "  results/NNS\n",
            "  demonstrated/VBD\n",
            "  that/IN\n",
            "  transformers/NNS\n",
            "  could/MD\n",
            "  compete/VB\n",
            "  with/IN\n",
            "  leading/JJ\n",
            "  CNNs/NNP\n",
            "  But/CC\n",
            "  Maithra/NNP\n",
            "  Raghu/NNP\n",
            "  (NP a/DT computer/NN)\n",
            "  (NP scientist/NN)\n",
            "  at/IN\n",
            "  Google/NNP\n",
            "  Brain/NNP\n",
            "  s/JJ\n",
            "  Mountain/NNP\n",
            "  View/NNP\n",
            "  (NP office/NN)\n",
            "  in/IN\n",
            "  California/NNP\n",
            "  wanted/VBD\n",
            "  to/TO\n",
            "  know/VB\n",
            "  if/IN\n",
            "  they/PRP\n",
            "  see/VBP\n",
            "  images/VBZ\n",
            "  (NP the/DT same/JJ way/NN)\n",
            "  CNNs/NNP\n",
            "  do/VBP\n",
            "  Neural/NNP\n",
            "  nets/NNS\n",
            "  are/VBP\n",
            "  notorious/JJ\n",
            "  for/IN\n",
            "  being/VBG\n",
            "  indecipherable/JJ\n",
            "  black/JJ\n",
            "  boxes/NNS\n",
            "  but/CC\n",
            "  there/EX\n",
            "  are/VBP\n",
            "  ways/NNS\n",
            "  to/TO\n",
            "  peek/VB\n",
            "  inside/RB\n",
            "  such/JJ\n",
            "  as/IN\n",
            "  by/IN\n",
            "  examining/VBG\n",
            "  (NP the/DT net/JJ s/NN)\n",
            "  (NP input/NN)\n",
            "  and/CC\n",
            "  (NP output/NN)\n",
            "  (NP layer/NN)\n",
            "  by/IN\n",
            "  (NP layer/NN)\n",
            "  to/TO\n",
            "  see/VB\n",
            "  how/WRB\n",
            "  (NP the/DT training/NN)\n",
            "  data/NNS\n",
            "  flows/VBZ\n",
            "  through/IN\n",
            "  Raghu/NNP\n",
            "  s/NNP\n",
            "  (NP group/NN)\n",
            "  did/VBD\n",
            "  essentially/RB\n",
            "  (NP this/DT picking/NN)\n",
            "  ViT/NNP\n",
            "  apart/RB\n",
            "  Her/NNP\n",
            "  (NP group/NN)\n",
            "  identified/VBD\n",
            "  ways/NNS\n",
            "  in/IN\n",
            "  which/WDT\n",
            "  (NP self/NN)\n",
            "  (NP attention/NN)\n",
            "  leads/VBZ\n",
            "  to/TO\n",
            "  a/DT\n",
            "  different/JJ\n",
            "  means/NNS\n",
            "  of/IN\n",
            "  (NP perception/NN)\n",
            "  within/IN\n",
            "  (NP the/DT algorithm/NN)\n",
            "  Ultimately/RB\n",
            "  (NP a/DT transformer/NN)\n",
            "  (NP s/NN)\n",
            "  (NP power/NN)\n",
            "  comes/VBZ\n",
            "  from/IN\n",
            "  (NP the/DT way/NN)\n",
            "  it/PRP\n",
            "  processes/VBZ\n",
            "  (NP the/DT encoded/JJ data/NN)\n",
            "  of/IN\n",
            "  (NP an/DT image/NN)\n",
            "  In/IN\n",
            "  CNNs/NNP\n",
            "  you/PRP\n",
            "  start/VBP\n",
            "  off/IN\n",
            "  being/VBG\n",
            "  very/RB\n",
            "  local/JJ\n",
            "  and/CC\n",
            "  slowly/RB\n",
            "  get/VB\n",
            "  (NP a/DT global/JJ perspective/NN)\n",
            "  said/VBD\n",
            "  Raghu/NNP\n",
            "  A/NNP\n",
            "  CNN/NNP\n",
            "  recognizes/VBZ\n",
            "  (NP an/DT image/NN)\n",
            "  (NP pixel/NN)\n",
            "  by/IN\n",
            "  (NP pixel/NN)\n",
            "  identifying/VBG\n",
            "  features/NNS\n",
            "  like/IN\n",
            "  corners/NNS\n",
            "  or/CC\n",
            "  lines/NNS\n",
            "  by/IN\n",
            "  building/VBG\n",
            "  its/PRP$\n",
            "  (NP way/NN)\n",
            "  up/IN\n",
            "  from/IN\n",
            "  the/DT\n",
            "  local/JJ\n",
            "  to/TO\n",
            "  the/DT\n",
            "  global/JJ\n",
            "  But/CC\n",
            "  in/IN\n",
            "  transformers/NNS\n",
            "  with/IN\n",
            "  (NP self/JJ attention/NN)\n",
            "  even/RB\n",
            "  the/DT\n",
            "  very/RB\n",
            "  (NP first/JJ layer/NN)\n",
            "  of/IN\n",
            "  (NP information/NN)\n",
            "  (NP processing/NN)\n",
            "  makes/VBZ\n",
            "  connections/NNS\n",
            "  between/IN\n",
            "  (NP distant/JJ image/NN)\n",
            "  locations/NNS\n",
            "  just/RB\n",
            "  as/IN\n",
            "  with/IN\n",
            "  (NP language/NN)\n",
            "  If/IN\n",
            "  a/DT\n",
            "  CNN/NNP\n",
            "  (NP s/NN)\n",
            "  (NP approach/NN)\n",
            "  is/VBZ\n",
            "  like/IN\n",
            "  starting/VBG\n",
            "  at/IN\n",
            "  (NP a/DT single/JJ pixel/NN)\n",
            "  and/CC\n",
            "  zooming/VBG\n",
            "  out/RP\n",
            "  (NP a/DT transformer/NN)\n",
            "  slowly/RB\n",
            "  brings/VBZ\n",
            "  (NP the/DT whole/JJ fuzzy/JJ image/NN)\n",
            "  into/IN\n",
            "  (NP focus/NN)\n",
            "  (NP This/DT difference/NN)\n",
            "  is/VBZ\n",
            "  simpler/VBN\n",
            "  to/TO\n",
            "  understand/VB\n",
            "  in/IN\n",
            "  (NP the/DT realm/NN)\n",
            "  of/IN\n",
            "  (NP language/NN)\n",
            "  where/WRB\n",
            "  transformers/NNS\n",
            "  were/VBD\n",
            "  first/RB\n",
            "  conceived/VBN\n",
            "  Consider/NNP\n",
            "  these/DT\n",
            "  sentences/NNS\n",
            "  (NP The/DT owl/NN)\n",
            "  spied/VBD\n",
            "  (NP a/DT squirrel/NN)\n",
            "  It/PRP\n",
            "  tried/VBD\n",
            "  to/TO\n",
            "  grab/VB\n",
            "  it/PRP\n",
            "  with/IN\n",
            "  its/PRP$\n",
            "  talons/NNS\n",
            "  but/CC\n",
            "  only/RB\n",
            "  got/VBD\n",
            "  (NP the/DT end/NN)\n",
            "  of/IN\n",
            "  its/PRP$\n",
            "  (NP tail/NN)\n",
            "  (NP The/DT structure/NN)\n",
            "  of/IN\n",
            "  (NP the/DT second/JJ sentence/NN)\n",
            "  is/VBZ\n",
            "  confusing/VBG\n",
            "  What/WP\n",
            "  do/VBP\n",
            "  those/DT\n",
            "  it/PRP\n",
            "  s/VBD\n",
            "  (NP refer/NN)\n",
            "  to/TO\n",
            "  A/NNP\n",
            "  CNN/NNP\n",
            "  that/WDT\n",
            "  focuses/VBZ\n",
            "  only/RB\n",
            "  on/IN\n",
            "  the/DT\n",
            "  words/NNS\n",
            "  immediately/RB\n",
            "  around/IN\n",
            "  the/DT\n",
            "  it/PRP\n",
            "  s/VBD\n",
            "  would/MD\n",
            "  struggle/VB\n",
            "  but/CC\n",
            "  (NP a/DT transformer/NN)\n",
            "  (NP connecting/NN)\n",
            "  (NP every/DT word/NN)\n",
            "  to/TO\n",
            "  (NP every/DT other/JJ word/NN)\n",
            "  could/MD\n",
            "  discern/VB\n",
            "  that/IN\n",
            "  (NP the/DT owl/NN)\n",
            "  did/VBD\n",
            "  (NP the/DT grabbing/NN)\n",
            "  and/CC\n",
            "  (NP the/DT squirrel/NN)\n",
            "  lost/VBD\n",
            "  (NP part/NN)\n",
            "  of/IN\n",
            "  its/PRP$\n",
            "  (NP tail/NN)\n",
            "  Now/RB\n",
            "  that/IN\n",
            "  it/PRP\n",
            "  was/VBD\n",
            "  clear/JJ\n",
            "  transformers/NNS\n",
            "  processed/VBD\n",
            "  images/NNS\n",
            "  fundamentally/RB\n",
            "  differently/RB\n",
            "  from/IN\n",
            "  convolutional/JJ\n",
            "  networks/NNS\n",
            "  researchers/NNS\n",
            "  only/RB\n",
            "  grew/VBD\n",
            "  more/RBR\n",
            "  excited/JJ\n",
            "  (NP The/DT transformer/NN)\n",
            "  (NP s/NN)\n",
            "  (NP versatility/NN)\n",
            "  in/IN\n",
            "  converting/VBG\n",
            "  data/NNS\n",
            "  from/IN\n",
            "  a/DT\n",
            "  one/CD\n",
            "  (NP dimensional/JJ string/NN)\n",
            "  like/IN\n",
            "  (NP a/DT sentence/NN)\n",
            "  into/IN\n",
            "  a/DT\n",
            "  two/CD\n",
            "  (NP dimensional/JJ array/NN)\n",
            "  like/IN\n",
            "  (NP an/DT image/NN)\n",
            "  suggests/VBZ\n",
            "  that/IN\n",
            "  such/PDT\n",
            "  (NP a/DT model/NN)\n",
            "  could/MD\n",
            "  handle/VB\n",
            "  data/NNS\n",
            "  of/IN\n",
            "  many/JJ\n",
            "  other/JJ\n",
            "  flavors/NNS\n",
            "  Wang/NNP\n",
            "  for/IN\n",
            "  (NP example/NN)\n",
            "  thinks/VBZ\n",
            "  (NP the/DT transformer/NN)\n",
            "  may/MD\n",
            "  be/VB\n",
            "  (NP a/DT big/JJ step/NN)\n",
            "  toward/IN\n",
            "  achieving/VBG\n",
            "  (NP a/DT kind/NN)\n",
            "  of/IN\n",
            "  (NP convergence/NN)\n",
            "  of/IN\n",
            "  neural/JJ\n",
            "  net/JJ\n",
            "  architectures/NNS\n",
            "  resulting/VBG\n",
            "  in/IN\n",
            "  (NP a/DT universal/JJ approach/NN)\n",
            "  to/TO\n",
            "  (NP computer/NN)\n",
            "  (NP vision/NN)\n",
            "  and/CC\n",
            "  perhaps/RB\n",
            "  to/TO\n",
            "  other/JJ\n",
            "  AI/NNP\n",
            "  tasks/NNS\n",
            "  as/RB\n",
            "  well/RB\n",
            "  There/EX\n",
            "  are/VBP\n",
            "  limitations/NNS\n",
            "  to/TO\n",
            "  making/VBG\n",
            "  it/PRP\n",
            "  really/RB\n",
            "  happen/VB\n",
            "  of/IN\n",
            "  (NP course/NN)\n",
            "  he/PRP\n",
            "  said/VBD\n",
            "  but/CC\n",
            "  if/IN\n",
            "  there/EX\n",
            "  is/VBZ\n",
            "  (NP a/DT model/NN)\n",
            "  that/WDT\n",
            "  can/MD\n",
            "  universalize/VB\n",
            "  where/WRB\n",
            "  you/PRP\n",
            "  can/MD\n",
            "  put/VB\n",
            "  all/DT\n",
            "  kinds/NNS\n",
            "  of/IN\n",
            "  data/NNS\n",
            "  in/IN\n",
            "  one/CD\n",
            "  (NP machine/NN)\n",
            "  then/RB\n",
            "  certainly/RB\n",
            "  that/IN\n",
            "  s/JJ\n",
            "  very/RB\n",
            "  fancy/JJ\n",
            "  Convergence/NNP\n",
            "  Coming/NNP\n",
            "  Now/RB\n",
            "  researchers/NNS\n",
            "  want/VBP\n",
            "  to/TO\n",
            "  apply/VB\n",
            "  transformers/NNS\n",
            "  to/TO\n",
            "  an/DT\n",
            "  even/RB\n",
            "  harder/RBR\n",
            "  (NP task/NN)\n",
            "  inventing/VBG\n",
            "  new/JJ\n",
            "  images/NNS\n",
            "  Language/VBP\n",
            "  tools/NNS\n",
            "  such/JJ\n",
            "  as/IN\n",
            "  GPT/NNP\n",
            "  can/MD\n",
            "  generate/VB\n",
            "  (NP new/JJ text/NN)\n",
            "  based/VBN\n",
            "  on/IN\n",
            "  their/PRP$\n",
            "  (NP training/NN)\n",
            "  data/NNS\n",
            "  In/IN\n",
            "  (NP a/DT paper/NN)\n",
            "  presented/VBN\n",
            "  (NP last/JJ year/NN)\n",
            "  Wang/NNP\n",
            "  combined/VBD\n",
            "  two/CD\n",
            "  (NP transformer/NN)\n",
            "  models/NNS\n",
            "  in/IN\n",
            "  (NP an/DT effort/NN)\n",
            "  to/TO\n",
            "  do/VB\n",
            "  the/DT\n",
            "  same/JJ\n",
            "  for/IN\n",
            "  images/VBZ\n",
            "  a/DT\n",
            "  much/RB\n",
            "  (NP harder/NN)\n",
            "  (NP problem/NN)\n",
            "  When/WRB\n",
            "  (NP the/DT double/JJ transformer/NN)\n",
            "  (NP network/NN)\n",
            "  trained/VBD\n",
            "  on/IN\n",
            "  the/DT\n",
            "  faces/VBZ\n",
            "  of/IN\n",
            "  more/JJR\n",
            "  than/IN\n",
            "  celebrities/NNS\n",
            "  it/PRP\n",
            "  synthesized/VBD\n",
            "  new/JJ\n",
            "  facial/JJ\n",
            "  images/NNS\n",
            "  at/IN\n",
            "  (NP moderate/JJ resolution/NN)\n",
            "  The/DT\n",
            "  invented/JJ\n",
            "  celebrities/NNS\n",
            "  are/VBP\n",
            "  impressively/RB\n",
            "  realistic/JJ\n",
            "  and/CC\n",
            "  at/IN\n",
            "  least/JJS\n",
            "  as/RB\n",
            "  convincing/JJ\n",
            "  as/IN\n",
            "  those/DT\n",
            "  created/VBN\n",
            "  by/IN\n",
            "  CNNs/NNP\n",
            "  according/VBG\n",
            "  to/TO\n",
            "  (NP the/DT inception/NN)\n",
            "  score/VBD\n",
            "  (NP a/DT standard/JJ way/NN)\n",
            "  of/IN\n",
            "  evaluating/VBG\n",
            "  images/NNS\n",
            "  generated/VBN\n",
            "  by/IN\n",
            "  (NP a/DT neural/JJ net/NN)\n",
            "  Wang/NNP\n",
            "  argues/VBZ\n",
            "  that/IN\n",
            "  (NP the/DT transformer/NN)\n",
            "  (NP s/NN)\n",
            "  (NP success/NN)\n",
            "  in/IN\n",
            "  generating/VBG\n",
            "  images/NNS\n",
            "  is/VBZ\n",
            "  even/RB\n",
            "  more/RBR\n",
            "  surprising/JJ\n",
            "  than/IN\n",
            "  ViT/NNP\n",
            "  (NP s/JJ prowess/NN)\n",
            "  in/IN\n",
            "  (NP image/NN)\n",
            "  (NP classification/NN)\n",
            "  A/NNP\n",
            "  (NP generative/JJ model/NN)\n",
            "  needs/NNS\n",
            "  to/TO\n",
            "  synthesize/VB\n",
            "  needs/NNS\n",
            "  to/TO\n",
            "  be/VB\n",
            "  able/JJ\n",
            "  to/TO\n",
            "  add/VB\n",
            "  (NP information/NN)\n",
            "  to/TO\n",
            "  look/VB\n",
            "  plausible/JJ\n",
            "  he/PRP\n",
            "  said/VBD\n",
            "  And/CC\n",
            "  as/IN\n",
            "  with/IN\n",
            "  (NP classification/NN)\n",
            "  the/DT\n",
            "  transformer/JJR\n",
            "  (NP approach/NN)\n",
            "  is/VBZ\n",
            "  replacing/VBG\n",
            "  convolutional/JJ\n",
            "  networks/NNS\n",
            "  Raghu/NNP\n",
            "  and/CC\n",
            "  Wang/NNP\n",
            "  see/VBP\n",
            "  potential/JJ\n",
            "  for/IN\n",
            "  new/JJ\n",
            "  uses/NNS\n",
            "  of/IN\n",
            "  transformers/NNS\n",
            "  in/IN\n",
            "  (NP multimodal/NN)\n",
            "  processing/VBG\n",
            "  (NP a/DT model/NN)\n",
            "  that/WDT\n",
            "  can/MD\n",
            "  simultaneously/RB\n",
            "  handle/VB\n",
            "  multiple/JJ\n",
            "  types/NNS\n",
            "  of/IN\n",
            "  data/NNS\n",
            "  like/IN\n",
            "  raw/JJ\n",
            "  images/NNS\n",
            "  (NP video/NN)\n",
            "  and/CC\n",
            "  (NP language/NN)\n",
            "  It/PRP\n",
            "  was/VBD\n",
            "  trickier/JJR\n",
            "  to/TO\n",
            "  do/VB\n",
            "  before/IN\n",
            "  Raghu/NNP\n",
            "  said/VBD\n",
            "  because/IN\n",
            "  of/IN\n",
            "  (NP that/DT siloed/NN)\n",
            "  (NP approach/NN)\n",
            "  where/WRB\n",
            "  (NP each/DT type/NN)\n",
            "  of/IN\n",
            "  data/NNS\n",
            "  had/VBD\n",
            "  its/PRP$\n",
            "  (NP own/JJ specialized/JJ model/NN)\n",
            "  But/CC\n",
            "  transformers/NNS\n",
            "  suggest/VBP\n",
            "  (NP a/DT way/NN)\n",
            "  to/TO\n",
            "  combine/VB\n",
            "  (NP multiple/JJ input/NN)\n",
            "  sources/NNS\n",
            "  There/EX\n",
            "  s/VBP\n",
            "  (NP a/DT whole/JJ realm/NN)\n",
            "  of/IN\n",
            "  interesting/JJ\n",
            "  applications/NNS\n",
            "  combining/VBG\n",
            "  some/DT\n",
            "  of/IN\n",
            "  these/DT\n",
            "  different/JJ\n",
            "  types/NNS\n",
            "  of/IN\n",
            "  data/NNS\n",
            "  and/CC\n",
            "  images/NNS\n",
            "  For/IN\n",
            "  (NP example/NN)\n",
            "  multimodal/NNS\n",
            "  networks/RB\n",
            "  might/MD\n",
            "  (NP power/NN)\n",
            "  (NP a/DT system/NN)\n",
            "  that/WDT\n",
            "  reads/VBZ\n",
            "  (NP a/DT person/NN)\n",
            "  s/JJ\n",
            "  lips/NNS\n",
            "  in/IN\n",
            "  (NP addition/NN)\n",
            "  to/TO\n",
            "  listening/VBG\n",
            "  to/TO\n",
            "  their/PRP$\n",
            "  (NP voice/NN)\n",
            "  You/PRP\n",
            "  could/MD\n",
            "  have/VB\n",
            "  (NP a/DT rich/JJ representation/NN)\n",
            "  of/IN\n",
            "  (NP both/DT language/NN)\n",
            "  and/CC\n",
            "  (NP image/NN)\n",
            "  (NP information/NN)\n",
            "  Raghu/NNP\n",
            "  said/VBD\n",
            "  and/CC\n",
            "  in/IN\n",
            "  a/DT\n",
            "  much/JJ\n",
            "  deeper/JJR\n",
            "  (NP way/NN)\n",
            "  than/IN\n",
            "  was/VBD\n",
            "  possible/JJ\n",
            "  before/IN\n",
            "  Emerging/VBG\n",
            "  (NP work/NN)\n",
            "  suggests/VBZ\n",
            "  (NP a/DT spectrum/NN)\n",
            "  of/IN\n",
            "  new/JJ\n",
            "  uses/NNS\n",
            "  for/IN\n",
            "  transformers/NNS\n",
            "  in/IN\n",
            "  other/JJ\n",
            "  AI/NNP\n",
            "  domains/NNS\n",
            "  including/VBG\n",
            "  teaching/VBG\n",
            "  robots/NNS\n",
            "  to/TO\n",
            "  recognize/VB\n",
            "  (NP human/JJ body/NN)\n",
            "  movements/NNS\n",
            "  training/VBG\n",
            "  machines/NNS\n",
            "  to/TO\n",
            "  discern/VB\n",
            "  emotions/NNS\n",
            "  in/IN\n",
            "  (NP speech/NN)\n",
            "  and/CC\n",
            "  detecting/VBG\n",
            "  (NP stress/NN)\n",
            "  levels/NNS\n",
            "  in/IN\n",
            "  electrocardiograms/JJ\n",
            "  (NP Another/DT program/NN)\n",
            "  with/IN\n",
            "  transformer/JJ\n",
            "  components/NNS\n",
            "  is/VBZ\n",
            "  AlphaFold/NNP\n",
            "  which/WDT\n",
            "  made/VBD\n",
            "  headlines/NNS\n",
            "  (NP last/JJ year/NN)\n",
            "  for/IN\n",
            "  its/PRP$\n",
            "  (NP ability/NN)\n",
            "  to/TO\n",
            "  quickly/RB\n",
            "  predict/VB\n",
            "  (NP protein/NN)\n",
            "  structures/VBZ\n",
            "  (NP a/DT task/NN)\n",
            "  that/WDT\n",
            "  used/VBD\n",
            "  to/TO\n",
            "  require/VB\n",
            "  (NP a/DT decade/NN)\n",
            "  of/IN\n",
            "  (NP intensive/JJ analysis/NN)\n",
            "  The/DT\n",
            "  Trade/NNP\n",
            "  Off/NNP\n",
            "  Even/RB\n",
            "  if/IN\n",
            "  transformers/NNS\n",
            "  can/MD\n",
            "  help/VB\n",
            "  unite/VB\n",
            "  and/CC\n",
            "  improve/VB\n",
            "  the/DT\n",
            "  tools/NNS\n",
            "  of/IN\n",
            "  AI/NNP\n",
            "  emerging/VBG\n",
            "  technologies/NNS\n",
            "  often/RB\n",
            "  come/VBP\n",
            "  at/IN\n",
            "  (NP a/DT steep/JJ cost/NN)\n",
            "  and/CC\n",
            "  this/DT\n",
            "  one/CD\n",
            "  is/VBZ\n",
            "  no/DT\n",
            "  different/JJ\n",
            "  A/NNP\n",
            "  (NP transformer/NN)\n",
            "  requires/VBZ\n",
            "  a/DT\n",
            "  higher/JJR\n",
            "  (NP outlay/NN)\n",
            "  of/IN\n",
            "  (NP computational/JJ power/NN)\n",
            "  in/IN\n",
            "  (NP the/DT pre/NN)\n",
            "  (NP training/NN)\n",
            "  (NP phase/NN)\n",
            "  before/IN\n",
            "  it/PRP\n",
            "  can/MD\n",
            "  beat/VB\n",
            "  (NP the/DT accuracy/NN)\n",
            "  of/IN\n",
            "  its/PRP$\n",
            "  conventional/JJ\n",
            "  competitors/NNS\n",
            "  That/WDT\n",
            "  could/MD\n",
            "  be/VB\n",
            "  (NP a/DT problem/NN)\n",
            "  (NP People/NN)\n",
            "  are/VBP\n",
            "  always/RB\n",
            "  getting/VBG\n",
            "  more/JJR\n",
            "  and/CC\n",
            "  more/RBR\n",
            "  interested/JJ\n",
            "  in/IN\n",
            "  (NP high/JJ resolution/NN)\n",
            "  images/NNS\n",
            "  Wang/NNP\n",
            "  said/VBD\n",
            "  That/IN\n",
            "  (NP training/NN)\n",
            "  (NP expense/NN)\n",
            "  could/MD\n",
            "  be/VB\n",
            "  (NP a/DT drawback/NN)\n",
            "  to/TO\n",
            "  (NP widespread/JJ implementation/NN)\n",
            "  of/IN\n",
            "  transformers/NNS\n",
            "  However/RB\n",
            "  Raghu/NNP\n",
            "  sees/VBZ\n",
            "  (NP the/DT training/NN)\n",
            "  (NP hurdle/NN)\n",
            "  as/IN\n",
            "  one/CD\n",
            "  that/WDT\n",
            "  can/MD\n",
            "  be/VB\n",
            "  overcome/VBN\n",
            "  simply/RB\n",
            "  enough/JJ\n",
            "  with/IN\n",
            "  sophisticated/JJ\n",
            "  filters/NNS\n",
            "  and/CC\n",
            "  other/JJ\n",
            "  tools/NNS\n",
            "  Wang/NNP\n",
            "  also/RB\n",
            "  points/VBZ\n",
            "  out/RP\n",
            "  that/IN\n",
            "  even/RB\n",
            "  though/IN\n",
            "  visual/JJ\n",
            "  transformers/NNS\n",
            "  have/VBP\n",
            "  ignited/VBN\n",
            "  new/JJ\n",
            "  efforts/NNS\n",
            "  to/TO\n",
            "  push/VB\n",
            "  AI/NNP\n",
            "  forward/RB\n",
            "  including/VBG\n",
            "  his/PRP$\n",
            "  own/JJ\n",
            "  many/JJ\n",
            "  of/IN\n",
            "  the/DT\n",
            "  new/JJ\n",
            "  models/NNS\n",
            "  still/RB\n",
            "  incorporate/VBP\n",
            "  the/DT\n",
            "  best/JJS\n",
            "  parts/NNS\n",
            "  of/IN\n",
            "  convolutions/NNS\n",
            "  That/WDT\n",
            "  means/VBZ\n",
            "  future/JJ\n",
            "  models/NNS\n",
            "  are/VBP\n",
            "  more/RBR\n",
            "  likely/JJ\n",
            "  to/TO\n",
            "  use/VB\n",
            "  both/DT\n",
            "  than/IN\n",
            "  to/TO\n",
            "  abandon/VB\n",
            "  CNNs/NNP\n",
            "  entirely/RB\n",
            "  he/PRP\n",
            "  says/VBZ\n",
            "  It/PRP\n",
            "  also/RB\n",
            "  suggests/VBZ\n",
            "  the/DT\n",
            "  tantalizing/VBG\n",
            "  (NP prospect/NN)\n",
            "  of/IN\n",
            "  (NP some/DT hybrid/JJ architecture/NN)\n",
            "  that/WDT\n",
            "  draws/VBZ\n",
            "  on/IN\n",
            "  the/DT\n",
            "  strengths/NNS\n",
            "  of/IN\n",
            "  transformers/NNS\n",
            "  in/IN\n",
            "  ways/NNS\n",
            "  that/IN\n",
            "  (NP today/NN)\n",
            "  s/VBP\n",
            "  researchers/NNS\n",
            "  can/MD\n",
            "  t/VB\n",
            "  predict/JJ\n",
            "  Perhaps/RB\n",
            "  we/PRP\n",
            "  shouldn/VBP\n",
            "  (NP t/JJ rush/NN)\n",
            "  to/TO\n",
            "  (NP the/DT conclusion/NN)\n",
            "  that/IN\n",
            "  (NP the/DT transformer/NN)\n",
            "  will/MD\n",
            "  be/VB\n",
            "  (NP the/DT final/JJ model/NN)\n",
            "  Wang/NNP\n",
            "  said/VBD\n",
            "  But/CC\n",
            "  it/PRP\n",
            "  s/VBZ\n",
            "  increasingly/RB\n",
            "  likely/JJ\n",
            "  that/IN\n",
            "  (NP the/DT transformer/NN)\n",
            "  will/MD\n",
            "  at/IN\n",
            "  least/JJS\n",
            "  be/VB\n",
            "  (NP a/DT part/NN)\n",
            "  of/IN\n",
            "  whatever/WDT\n",
            "  (NP new/JJ super/JJ tool/NN)\n",
            "  comes/VBZ\n",
            "  to/TO\n",
            "  an/DT\n",
            "  AI/NNP\n",
            "  (NP shop/NN)\n",
            "  near/IN\n",
            "  you/PRP)\n",
            "(NP local/JJ hardware/NN)\n",
            "(NP store/NN)\n",
            "(NP a/DT new/JJ kind/NN)\n",
            "(NP hammer/NN)\n",
            "(NP the/DT shelf/NN)\n",
            "(NP this/DT hammer/NN)\n",
            "(NP an/DT attachment/NN)\n",
            "(NP a/DT twist/NN)\n",
            "(NP the/DT tool/NN)\n",
            "(NP a/DT saw/NN)\n",
            "(NP any/DT other/JJ option/NN)\n",
            "(NP fact/NN)\n",
            "(NP tool/NN)\n",
            "(NP development/NN)\n",
            "(NP this/DT hammer/NN)\n",
            "(NP the/DT convergence/NN)\n",
            "(NP a/DT single/JJ device/NN)\n",
            "(NP similar/JJ story/NN)\n",
            "(NP artificial/JJ intelligence/NN)\n",
            "(NP versatile/JJ new/JJ hammer/NN)\n",
            "(NP a/DT kind/NN)\n",
            "(NP artificial/JJ neural/JJ network/NN)\n",
            "(NP a/DT network/NN)\n",
            "(NP some/DT task/NN)\n",
            "(NP a/DT transformer/NN)\n",
            "(NP language/NN)\n",
            "(NP The/DT transformer/NN)\n",
            "(NP a/DT paper/NN)\n",
            "(NP the/DT system/NN)\n",
            "(NP input/NN)\n",
            "(NP the/DT whole/NN)\n",
            "(NP a/DT language/NN)\n",
            "(NP model/NN)\n",
            "(NP example/NN)\n",
            "(NP The/DT transformer/NN)\n",
            "(NP contrast/NN)\n",
            "(NP every/DT element/NN)\n",
            "(NP the/DT input/NN)\n",
            "(NP attention/NN)\n",
            "(NP every/DT other/JJ element/NN)\n",
            "(NP self/NN)\n",
            "(NP attention/NN)\n",
            "(NP the/DT transformer/NN)\n",
            "(NP set/NN)\n",
            "(NP progress/NN)\n",
            "(NP language/NN)\n",
            "(NP this/DT deep/JJ learning/NN)\n",
            "(NP revolution/NN)\n",
            "(NP natural/JJ language/NN)\n",
            "(NP processing/NN)\n",
            "(NP sort/NN)\n",
            "(NP a/DT latecomer/NN)\n",
            "(NP the/DT computer/NN)\n",
            "(NP scientist/NN)\n",
            "(NP a/DT sense/NN)\n",
            "(NP computer/NN)\n",
            "(NP vision/NN)\n",
            "(NP the/DT front/NN)\n",
            "(NP runner/NN)\n",
            "(NP word/NN)\n",
            "(NP recognition/NN)\n",
            "(NP focus/NN)\n",
            "(NP text/NN)\n",
            "(NP a/DT wave/NN)\n",
            "(NP consistent/JJ new/JJ text/NN)\n",
            "(NP an/DT unsettling/JJ degree/NN)\n",
            "(NP The/DT success/NN)\n",
            "(NP crowd/NN)\n",
            "(NP The/DT answer/NN)\n",
            "(NP versatile/NN)\n",
            "(NP some/DT vision/NN)\n",
            "(NP image/NN)\n",
            "(NP classification/NN)\n",
            "(NP work/NN)\n",
            "(NP input/NN)\n",
            "(NP machine/NN)\n",
            "(NP learning/NN)\n",
            "(NP computer/NN)\n",
            "(NP vision/NN)\n",
            "(NP computer/NN)\n",
            "(NP vision/NN)\n",
            "(NP the/DT arrival/NN)\n",
            "(NP the/DT possibility/NN)\n",
            "(NP a/DT convergence/NN)\n",
            "(NP the/DT transformer/NN)\n",
            "(NP universal/NN)\n",
            "(NP the/DT computer/NN)\n",
            "(NP scientist/NN)\n",
            "(NP good/JJ reason/NN)\n",
            "(NP the/DT entire/JJ spectrum/NN)\n",
            "(NP the/DT range/NN)\n",
            "(NP the/DT release/NN)\n",
            "(NP a/DT computer/NN)\n",
            "(NP scientist/NN)\n",
            "(NP computer/NN)\n",
            "(NP vision/NN)\n",
            "(NP everyone/NN)\n",
            "(NP the/DT field/NN)\n",
            "(NP deep/JJ learning/NN)\n",
            "(NP computer/NN)\n",
            "(NP vision/NN)\n",
            "(NP work/NN)\n",
            "(NP an/DT image/NN)\n",
            "(NP a/DT recognition/NN)\n",
            "(NP library/NN)\n",
            "(NP an/DT avocado/JJ apart/NN)\n",
            "(NP a/DT cloud/NN)\n",
            "(NP vision/NN)\n",
            "(NP the/DT field/NN)\n",
            "(NP resolution/NN)\n",
            "(NP the/DT processing/NN)\n",
            "(NP time/NN)\n",
            "(NP the/DT previous/JJ go/NN)\n",
            "(NP task/NN)\n",
            "(NP language/NN)\n",
            "(NP something/NN)\n",
            "(NP vision/NN)\n",
            "(NP The/DT idea/NN)\n",
            "(NP a/DT certain/JJ kind/NN)\n",
            "(NP sense/NN)\n",
            "(NP The/DT eventual/JJ result/NN)\n",
            "(NP a/DT network/NN)\n",
            "(NP a/DT conference/NN)\n",
            "(NP The/DT architecture/NN)\n",
            "(NP the/DT model/NN)\n",
            "(NP the/DT first/JJ transformer/NN)\n",
            "(NP a/DT lot/NN)\n",
            "(NP the/DT image/NN)\n",
            "(NP team/NN)\n",
            "(NP the/DT language/NN)\n",
            "(NP approach/NN)\n",
            "(NP self/JJ attention/NN)\n",
            "(NP every/DT pixel/NN)\n",
            "(NP time/NN)\n",
            "(NP image/NN)\n",
            "(NP The/DT size/NN)\n",
            "(NP the/DT resolution/NN)\n",
            "(NP the/DT original/JJ image/NN)\n",
            "(NP the/DT default/NN)\n",
            "(NP a/DT side/NN)\n",
            "(NP attention/NN)\n",
            "(NP enormous/JJ training/NN)\n",
            "(NP The/DT transformer/NN)\n",
            "(NP accuracy/NN)\n",
            "(NP result/NN)\n",
            "(NP anything/NN)\n",
            "(NP the/DT top/NN)\n",
            "(NP the/DT pack/NN)\n",
            "(NP classification/NN)\n",
            "(NP challenge/NN)\n",
            "(NP a/DT seminal/JJ image/NN)\n",
            "(NP recognition/NN)\n",
            "(NP contest/NN)\n",
            "(NP s/NN)\n",
            "(NP success/NN)\n",
            "(NP t/NN)\n",
            "(NP computer/NN)\n",
            "(NP vision/NN)\n",
            "(NP vision/NN)\n",
            "(NP the/DT midterm/JJ future/NN)\n",
            "(NP self/JJ attention/NN)\n",
            "(NP image/NN)\n",
            "(NP classification/NN)\n",
            "(NP database/NN)\n",
            "(NP the/DT start/NN)\n",
            "(NP an/DT updated/JJ version/NN)\n",
            "(NP approach/NN)\n",
            "(NP a/DT computer/NN)\n",
            "(NP scientist/NN)\n",
            "(NP office/NN)\n",
            "(NP the/DT same/JJ way/NN)\n",
            "(NP the/DT net/JJ s/NN)\n",
            "(NP input/NN)\n",
            "(NP output/NN)\n",
            "(NP layer/NN)\n",
            "(NP layer/NN)\n",
            "(NP the/DT training/NN)\n",
            "(NP group/NN)\n",
            "(NP this/DT picking/NN)\n",
            "(NP group/NN)\n",
            "(NP self/NN)\n",
            "(NP attention/NN)\n",
            "(NP perception/NN)\n",
            "(NP the/DT algorithm/NN)\n",
            "(NP a/DT transformer/NN)\n",
            "(NP s/NN)\n",
            "(NP power/NN)\n",
            "(NP the/DT way/NN)\n",
            "(NP the/DT encoded/JJ data/NN)\n",
            "(NP an/DT image/NN)\n",
            "(NP a/DT global/JJ perspective/NN)\n",
            "(NP an/DT image/NN)\n",
            "(NP pixel/NN)\n",
            "(NP pixel/NN)\n",
            "(NP way/NN)\n",
            "(NP self/JJ attention/NN)\n",
            "(NP first/JJ layer/NN)\n",
            "(NP information/NN)\n",
            "(NP processing/NN)\n",
            "(NP distant/JJ image/NN)\n",
            "(NP language/NN)\n",
            "(NP s/NN)\n",
            "(NP approach/NN)\n",
            "(NP a/DT single/JJ pixel/NN)\n",
            "(NP a/DT transformer/NN)\n",
            "(NP the/DT whole/JJ fuzzy/JJ image/NN)\n",
            "(NP focus/NN)\n",
            "(NP This/DT difference/NN)\n",
            "(NP the/DT realm/NN)\n",
            "(NP language/NN)\n",
            "(NP The/DT owl/NN)\n",
            "(NP a/DT squirrel/NN)\n",
            "(NP the/DT end/NN)\n",
            "(NP tail/NN)\n",
            "(NP The/DT structure/NN)\n",
            "(NP the/DT second/JJ sentence/NN)\n",
            "(NP refer/NN)\n",
            "(NP a/DT transformer/NN)\n",
            "(NP connecting/NN)\n",
            "(NP every/DT word/NN)\n",
            "(NP every/DT other/JJ word/NN)\n",
            "(NP the/DT owl/NN)\n",
            "(NP the/DT grabbing/NN)\n",
            "(NP the/DT squirrel/NN)\n",
            "(NP part/NN)\n",
            "(NP tail/NN)\n",
            "(NP The/DT transformer/NN)\n",
            "(NP s/NN)\n",
            "(NP versatility/NN)\n",
            "(NP dimensional/JJ string/NN)\n",
            "(NP a/DT sentence/NN)\n",
            "(NP dimensional/JJ array/NN)\n",
            "(NP an/DT image/NN)\n",
            "(NP a/DT model/NN)\n",
            "(NP example/NN)\n",
            "(NP the/DT transformer/NN)\n",
            "(NP a/DT big/JJ step/NN)\n",
            "(NP a/DT kind/NN)\n",
            "(NP convergence/NN)\n",
            "(NP a/DT universal/JJ approach/NN)\n",
            "(NP computer/NN)\n",
            "(NP vision/NN)\n",
            "(NP course/NN)\n",
            "(NP a/DT model/NN)\n",
            "(NP machine/NN)\n",
            "(NP task/NN)\n",
            "(NP new/JJ text/NN)\n",
            "(NP training/NN)\n",
            "(NP a/DT paper/NN)\n",
            "(NP last/JJ year/NN)\n",
            "(NP transformer/NN)\n",
            "(NP an/DT effort/NN)\n",
            "(NP harder/NN)\n",
            "(NP problem/NN)\n",
            "(NP the/DT double/JJ transformer/NN)\n",
            "(NP network/NN)\n",
            "(NP moderate/JJ resolution/NN)\n",
            "(NP the/DT inception/NN)\n",
            "(NP a/DT standard/JJ way/NN)\n",
            "(NP a/DT neural/JJ net/NN)\n",
            "(NP the/DT transformer/NN)\n",
            "(NP s/NN)\n",
            "(NP success/NN)\n",
            "(NP s/JJ prowess/NN)\n",
            "(NP image/NN)\n",
            "(NP classification/NN)\n",
            "(NP generative/JJ model/NN)\n",
            "(NP information/NN)\n",
            "(NP classification/NN)\n",
            "(NP approach/NN)\n",
            "(NP multimodal/NN)\n",
            "(NP a/DT model/NN)\n",
            "(NP video/NN)\n",
            "(NP language/NN)\n",
            "(NP that/DT siloed/NN)\n",
            "(NP approach/NN)\n",
            "(NP each/DT type/NN)\n",
            "(NP own/JJ specialized/JJ model/NN)\n",
            "(NP a/DT way/NN)\n",
            "(NP multiple/JJ input/NN)\n",
            "(NP a/DT whole/JJ realm/NN)\n",
            "(NP example/NN)\n",
            "(NP power/NN)\n",
            "(NP a/DT system/NN)\n",
            "(NP a/DT person/NN)\n",
            "(NP addition/NN)\n",
            "(NP voice/NN)\n",
            "(NP a/DT rich/JJ representation/NN)\n",
            "(NP both/DT language/NN)\n",
            "(NP image/NN)\n",
            "(NP information/NN)\n",
            "(NP way/NN)\n",
            "(NP work/NN)\n",
            "(NP a/DT spectrum/NN)\n",
            "(NP human/JJ body/NN)\n",
            "(NP speech/NN)\n",
            "(NP stress/NN)\n",
            "(NP Another/DT program/NN)\n",
            "(NP last/JJ year/NN)\n",
            "(NP ability/NN)\n",
            "(NP protein/NN)\n",
            "(NP a/DT task/NN)\n",
            "(NP a/DT decade/NN)\n",
            "(NP intensive/JJ analysis/NN)\n",
            "(NP a/DT steep/JJ cost/NN)\n",
            "(NP transformer/NN)\n",
            "(NP outlay/NN)\n",
            "(NP computational/JJ power/NN)\n",
            "(NP the/DT pre/NN)\n",
            "(NP training/NN)\n",
            "(NP phase/NN)\n",
            "(NP the/DT accuracy/NN)\n",
            "(NP a/DT problem/NN)\n",
            "(NP People/NN)\n",
            "(NP high/JJ resolution/NN)\n",
            "(NP training/NN)\n",
            "(NP expense/NN)\n",
            "(NP a/DT drawback/NN)\n",
            "(NP widespread/JJ implementation/NN)\n",
            "(NP the/DT training/NN)\n",
            "(NP hurdle/NN)\n",
            "(NP prospect/NN)\n",
            "(NP some/DT hybrid/JJ architecture/NN)\n",
            "(NP today/NN)\n",
            "(NP t/JJ rush/NN)\n",
            "(NP the/DT conclusion/NN)\n",
            "(NP the/DT transformer/NN)\n",
            "(NP the/DT final/JJ model/NN)\n",
            "(NP the/DT transformer/NN)\n",
            "(NP a/DT part/NN)\n",
            "(NP new/JJ super/JJ tool/NN)\n",
            "(NP shop/NN)\n"
          ]
        }
      ]
    }
  ]
}